{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, sys\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset_dir = './train/'\n",
    "test_dataset_dir = './test/'\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = None\n",
    "test_df = None\n",
    "train_files = listdir(train_dataset_dir)\n",
    "test_files = listdir(test_dataset_dir)\n",
    "\n",
    "train_df = pd.read_csv(train_dataset_dir + 'title_len.csv')\n",
    "test_df = pd.read_csv(test_dataset_dir + 'title_len.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in train_files:\n",
    "        s = f[:-4]\n",
    "        temp_df = pd.read_csv(train_dataset_dir + s + '.csv', lineterminator='\\n')\n",
    "        train_df[s] = temp_df[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in test_files:\n",
    "        s = f[:-4]\n",
    "        temp_df = pd.read_csv(test_dataset_dir + s + '.csv', lineterminator='\\n')\n",
    "        test_df[s] = temp_df[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['h2'] = train_df['h2'].fillna('')\n",
    "test_df['h2'] = test_df['h2'].fillna('')\n",
    "test_df['article'] = test_df['article'].fillna('')\n",
    "train_df['author'] = train_df['author'].fillna('')\n",
    "test_df['author'] = test_df['author'].fillna('')\n",
    "train_df['title'] = train_df['title'].fillna('')\n",
    "test_df['title'] = test_df['title'].fillna('')\n",
    "train_df['h1'] = train_df['h1'].fillna('') + train_df['author']\n",
    "test_df['h1'] = test_df['h1'].fillna('') + test_df['author']\n",
    "train_df['topics'] = train_df['topics'].fillna('')\n",
    "test_df['topics'] = test_df['topics'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5):\n",
    "        self.author_pop_dict = {}\n",
    "        self.author_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        \n",
    "    def get_author_pop(self, author_arr):\n",
    "        total_mean = []\n",
    "        for i in author_arr:\n",
    "            if i in self.author_pop_dict.keys():\n",
    "                total_mean.append(self.author_pop_dict[i])\n",
    "            else:\n",
    "                total_mean.append(self.author_pop_mean)\n",
    "        return total_mean\n",
    "\n",
    "    def get_train_author_pop(self, author_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for author, label in zip(author_arr, label_arr):\n",
    "            if author in popularity.keys():\n",
    "                popularity[author].append(int(label))\n",
    "            else:\n",
    "                popularity[author] = [int(label)]\n",
    "\n",
    "        delete_keys = []\n",
    "        for i in popularity.keys():\n",
    "            if(len(popularity[i]) < self.gate):\n",
    "                delete_keys.append(i)\n",
    "            else:\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "                \n",
    "        for j in delete_keys:\n",
    "            popularity.pop(j, None)\n",
    "        total_mean = []\n",
    "        cnt = 0\n",
    "        for i in author_arr:\n",
    "            if i in popularity.keys():\n",
    "                total_mean.append(popularity[i])\n",
    "            else:\n",
    "                cnt = cnt + 1\n",
    "                total_mean.append(-1)\n",
    "\n",
    "        mean = (sum(total_mean) + cnt)/ (len(total_mean) - cnt)\n",
    "        return popularity, mean\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.author_pop_dict, self.author_pop_mean = self.get_train_author_pop(X['author'], X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['popularity'] = self.get_author_pop(X['author'])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5):\n",
    "        self.author_pop_dict = {}\n",
    "        self.author_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        \n",
    "    def get_author_pop(self, author_arr):\n",
    "        total_mean = []\n",
    "        for i in author_arr:\n",
    "            if i in self.author_pop_dict.keys():\n",
    "                total_mean.append(self.author_pop_dict[i])\n",
    "            else:\n",
    "                total_mean.append(self.author_pop_mean)\n",
    "        return total_mean\n",
    "\n",
    "    def get_train_author_pop(self, author_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for author, label in zip(author_arr, label_arr):\n",
    "            if author in popularity.keys():\n",
    "                popularity[author].append(int(label))\n",
    "            else:\n",
    "                popularity[author] = [int(label)]\n",
    "\n",
    "        delete_keys = []\n",
    "        for i in popularity.keys():\n",
    "            if(len(popularity[i]) < self.gate):\n",
    "                delete_keys.append(i)\n",
    "            else:\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "                \n",
    "        for j in delete_keys:\n",
    "            popularity.pop(j, None)\n",
    "        total_mean = []\n",
    "        cnt = 0\n",
    "        for i in author_arr:\n",
    "            if i in popularity.keys():\n",
    "                total_mean.append(popularity[i])\n",
    "            else:\n",
    "                cnt = cnt + 1\n",
    "                total_mean.append(-1)\n",
    "\n",
    "        mean = (sum(total_mean) + cnt)/ (len(total_mean) - cnt)\n",
    "        return popularity, mean\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.author_pop_dict, self.author_pop_mean = self.get_train_author_pop(X['channel'], X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['channel_pop'] = self.get_author_pop(X['channel'])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorTopicsPopTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5, tps_list = []):\n",
    "        self.author_pop_dict = {}\n",
    "        self.author_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        self.df = None\n",
    "        self.tps_list = tps_list\n",
    "        \n",
    "    def get_author_channel_pop(self, channel_arr, author_arr, author_pop_arr):\n",
    "        arr = []\n",
    "        len_arr = []\n",
    "        for author, channel, author_pop in zip(author_arr, channel_arr, author_pop_arr):\n",
    "            author_channel_df = self.df.loc[self.df['author'] == author]\n",
    "            author_channel_df = author_channel_df.loc[author_channel_df['choose_tps'] == channel] \n",
    "            if (len(author_channel_df) < self.gate) or (channel not in self.tps_list):\n",
    "                arr.append(author_pop)\n",
    "                len_arr.append(len(author_channel_df))\n",
    "            else:\n",
    "                positive = len(author_channel_df.loc[author_channel_df['label'] == 1])\n",
    "                total = len(author_channel_df)\n",
    "                arr.append(positive/total)\n",
    "                len_arr.append(total)\n",
    "        return arr, len_arr\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        self.df = X.loc[:, ['author', 'choose_tps', 'label']] \n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['author_tps_pop'],  X['author_tps_count'] = self.get_author_channel_pop(X['choose_tps'],  X['author'],  X['popularity'])\n",
    "        if 'label' in X.columns:\n",
    "            X= X.drop(['author','choose_tps', 'label'], axis=1)\n",
    "        else:\n",
    "            X= X.drop(['author', 'choose_tps'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicsTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5, tps_list = []):\n",
    "        self.topics_pop_dict = {}\n",
    "        self.topics_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        self.tps_list = tps_list\n",
    "        \n",
    "    def get_topics_pop(self,topics_arr):\n",
    "        total_mean = []\n",
    "        choose_topic = []\n",
    "        for topics in topics_arr :\n",
    "            tps_val = -1\n",
    "            choose_tps = ''\n",
    "            for tps in topics.split():\n",
    "                if tps in self.topics_pop_dict.keys():\n",
    "                    if tps_val < self.topics_pop_dict[tps]:\n",
    "                        tps_val = self.topics_pop_dict[tps]\n",
    "                        choose_tps = tps\n",
    "            if topics == '' or choose_tps == '':\n",
    "                total_mean.append(self.topics_pop_mean)\n",
    "                choose_topic.append('NONE')\n",
    "            else:\n",
    "                total_mean.append(self.topics_pop_dict[choose_tps])\n",
    "                choose_topic.append(choose_tps)\n",
    "        return total_mean, choose_topic\n",
    "\n",
    "    def get_train_topics_pop(self, topics_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for i in self.tps_list:\n",
    "            popularity[i] = []\n",
    "        for topics, label in zip(topics_arr, label_arr):\n",
    "            for tps in topics.split():\n",
    "                if tps in popularity.keys():\n",
    "                    popularity[tps].append(int(label))\n",
    "        for i in popularity.keys():\n",
    "            if len(popularity[i]) != 0:\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "            else: \n",
    "                print(i)\n",
    "        \n",
    "        total_mean = []\n",
    "        cnt_non_tps = 0\n",
    "        for topics in topics_arr:\n",
    "            tps_val = -1\n",
    "            choose_tps = ''\n",
    "            for tps in topics.split():\n",
    "                if tps not in popularity.keys():\n",
    "                    continue\n",
    "                if tps_val < popularity[tps]:\n",
    "                    tps_val = popularity[tps]\n",
    "                    choose_tps = tps\n",
    "            if topics == '' or choose_tps == '':\n",
    "                cnt_non_tps += 1\n",
    "                total_mean.append(-1)\n",
    "            else:\n",
    "                total_mean.append(popularity[choose_tps])\n",
    "\n",
    "        mean = (sum(total_mean) + cnt_non_tps) / (len(total_mean) - cnt_non_tps)\n",
    "        return popularity, mean\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.topics_pop_dict, self.topics_pop_mean = self.get_train_topics_pop(X['topics'].fillna(\"\"), X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['topics_pop'] , X['choose_tps'] = self.get_topics_pop(X['topics'].fillna(\"\"))\n",
    "        X= X.drop(['topics'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorChannelPopTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5):\n",
    "        self.author_channel_pop_dict = {}\n",
    "        self.author_channel_pop_mean = 0\n",
    "        self.author_channel_cnt_dict = {}\n",
    "        self.author_channel_cnt_mean = 0\n",
    "        self.gate = 5\n",
    "        self.df = None\n",
    "        \n",
    "    def get_author_channel_pop(self, channel_arr, author_arr):\n",
    "        arr = []\n",
    "        len_arr = []\n",
    "        for author, channel in zip(author_arr, channel_arr):\n",
    "            if (author, channel) not in self.author_channel_pop_dict.keys():\n",
    "                arr.append(self.author_channel_pop_mean)\n",
    "                len_arr.append(self.author_channel_cnt_mean)\n",
    "            else:\n",
    "                arr.append(self.author_channel_pop_dict[(author, channel)])\n",
    "                len_arr.append(self.author_channel_cnt_dict[(author, channel)])\n",
    "        return arr, len_arr\n",
    "    \n",
    "    def get_train_author_channel_pop(self, channel_arr, author_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for author, channel, label in zip(author_arr, channel_arr, label_arr):\n",
    "            if (author, channel) in popularity.keys():\n",
    "                popularity[(author, channel)].append(int(label))\n",
    "            else:\n",
    "                popularity[(author, channel)] = [int(label)]\n",
    "            \n",
    "\n",
    "        delete_keys = []\n",
    "        cnt_mean = []\n",
    "        cnt_dict = {}\n",
    "        for i in popularity.keys():\n",
    "            if len(popularity[i]) < self.gate:\n",
    "                delete_keys.append(i)\n",
    "            else:\n",
    "                cnt_dict[i] = len(popularity[i])\n",
    "                cnt_mean.append(len(popularity[i]))\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "        for j in delete_keys:\n",
    "            popularity.pop(j, None)\n",
    "\n",
    "        total_mean = []\n",
    "        cnt_non_author_channel = 0\n",
    "        mean_cnt = 0\n",
    "        for author, channel in zip(author_arr, channel_arr):\n",
    "            if (author, channel) in popularity.keys():\n",
    "                total_mean.append(popularity[(author, channel)])\n",
    "            else:\n",
    "                cnt_non_author_channel += 1\n",
    "                total_mean.append(-1)\n",
    "        \n",
    "        mean = (sum(total_mean) + cnt_non_author_channel) / (len(total_mean) - cnt_non_author_channel)\n",
    "        mean_cnt = sum(cnt_mean) / len(cnt_mean)\n",
    "        return popularity, mean, cnt_dict, mean_cnt\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        self.author_channel_pop_dict, self.author_channel_pop_mean, self.author_channel_cnt_dict, self.author_channel_cnt_mean = self.get_train_author_channel_pop(X['channel'], X['author'], X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['author_channel_pop'],  X['author_channel_count'] = self.get_author_channel_pop(X['channel'], X['author'])\n",
    "        X= X.drop(['channel'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dic  = ['apps', 'facebook', 'design', 'photography', 'youtube', 'google', 'twitter', 'family', 'television', 'gallery', 'movies', 'ukraine', 'search', 'thrones', 'contributor', 'glass', 'humor', 'space', 'health', 'netflix', 'wars', 'east', 'linkedin', 'climate', 'cup', 'memes', 'reddit', 'winter', 'smartphone', 'vine', 'spotify', 'holiday', 'celebrities', 'yahoo', 'gmail', 'snapchat', 'diy', 'videos', 'launchpad', 'choice', 'earth', '2014', '360', 'chrome', 'shopping', 'california', 'tablet', 'small', 'ebay', 'game', 'house', 'instagram', 'holidays', 'web', 'snow', 'streaming', 'cyrus', 'late', 'lego', 'week', 'doctor', 'motorola', 'sxsw', 'reviews', 'parenting', 'nostalgia', 'rift', 'infographics', 'pandora', \"valentine's\", 'freedom', 'emmys', 'doodle', 'roundup', 'reality', 'angry', 'healthcare', 'snowden', 'gifs', 'valley', 'digital', 'beer', 'mad', 'motion', 'marathon', 'security', 'cover', 'journalism', 'features', 'bill', 'gates', 'parody', 'selfie', 'sunday', 'brands', 'beauty', 'star', 'toys', 'iphone', 'interviews', 'cases', 'solutions', 'america', 'augmented', 'human', 'now', 'recent', 'ibm', 'gift', 'crash', 'tourism', 'mars', 'electronics', 'green', 'williams', 'life', 'kickstarter', 'vimeo', 'man', 'academy', 'death', 'fashion', 'texting', 'tips', 'resume', 'xbox', 'storage', 'touch', 'kids', 'olympic', 'facts', 'siri', 'spring', 'at&t', 'hbo', 'forum', 'kingdom', 'nest', 'south', 'arrested', 'iwatch', 'los', 'anonymous', 'germany', 'opinion', 'use', 'scrivan', 'european', 'email', 'travel', 'venezuela', 'smartwatch', 'moon', 'international', 'police', 'jerry', 'costumes', 'history', 'psychology', 'congress', 'cyberbullying', 'pope', 'awards', 'lawsuit', 'hacks', 'study', 'drone', 'smart', 'easter', 'ferguson', 'ted', 'width', 'cbs', 'dna', 'management', 'teens', 'olympics', 'safety', 'stem', 'yelp', 'buzzwords', 'superheroes', 'legos', 'brazil', 'cable', 'disease', 'campaign', 'paris', 'perry', 'battery', 'crafts', 'gold', 'new', 'printers', 'iron', 'bad', 'pew', 'crime', 'news', 'online', 'korea', 'art', 'extreme', 'waze', 'fox', 'dogs', 'texas', 'retina', 'seinfeld', 'hangouts', 's5', 'cute', 'box', 'future', 'wikipedia', 'team', 'animated', 'jose', 'mashups', 'living', 'ocean', 'productivity', 'true', 'voice', 'speech', 'sam', 'fov', 'password', 'thanksgiving', 'seo', 'beat', 'deaths', 'internet', 'hashtags', 'conan', 'franco', 'smartwatches', 'philippines', 'graph', 'shazam', 'recalls', 'cameras', 'hubble', 'content', 'law', 'muppets', 'wireless', 'alibaba', 'fiber', 'browser', 'metrics', 'zelda', 'oliver', 'students', 'station', 'watson', 'printing', 'rss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AuthorProblem(DF, mode):\n",
    "    if mode == 'train':\n",
    "        new_df = pd.DataFrame([], columns = DF.columns)\n",
    "        for index, row in DF.iterrows(): \n",
    "            morethanone = False\n",
    "            for i in range(len(row['author'].split())):\n",
    "                if(row['author'].find(',')):\n",
    "                    a = row['author'].find(',')\n",
    "                    row['author'] = row['author'][:a] + \" \" + row['author'][a:]\n",
    "                if row['author'].split()[i] == 'and' or row['author'].split()[i] == '&' or row['author'].split()[i] == ',':\n",
    "                    morethanone = True\n",
    "                    a = row.copy(deep = True)\n",
    "                    if(i + 2 < len(row['author'].split())):\n",
    "                        a.loc['author'] = row['author'].split()[i+1] + ' ' + row['author'].split()[i+2]\n",
    "                        new_df = new_df.append(a)\n",
    "        DF = pd.concat([DF, new_df], ignore_index=True)\n",
    "    elif mode == 'test':\n",
    "        for index, row in DF.iterrows():\n",
    "            morethanone = False\n",
    "            for i in range(len(row['author'].split())):\n",
    "                if(row['author'].find(',')):\n",
    "                    a = row['author'].find(',')\n",
    "                    row['author'] = row['author'][:a] + \" \" + row['author'][a:]\n",
    "                if row['author'].split()[i] == 'and' or row['author'].split()[i] == '&' or row['author'].split()[i] == ',':\n",
    "                    morethanone = True\n",
    "                    break\n",
    "            if(morethanone):\n",
    "                if len(row['author'].split()) > 1:\n",
    "                    DF.loc[index,'author'] = row['author'].split()[0] + ' ' + row['author'].split()[1]\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_solve = AuthorProblem(train_df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df_solve[['dayhour', 'author_post', 'links', 'popularity', 'article_len', 'topics', 'channel', 'author', 'label']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, X_train['label'], test_size=0.3, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5486324401332469\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipe1 = Pipeline([('author', AuthorTransFormer(gate = 2)),\n",
    "                  ('channel', ChannelTransFormer(gate = 2)),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer(gate = 2)),\n",
    "                  ('topics', TopicsTransFormer(tps_list = topic_dic)),\n",
    "                  ('topics_author', AuthorTopicsPopTransFormer(tps_list = topic_dic)),\n",
    "                  ('std', StandardScaler()),\n",
    "                  (\"pca\", PCA()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.001,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipe1.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipe1.predict_proba(X_val)[:,1])\n",
    "print(score)\n",
    "pickle.dump(pipe1, open('featuresonly.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "================================== TEXT ===================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stanl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def preprocessor(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_Xtrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-92de4405d506>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtfidf_train_channel_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer_stem_nostop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtfidf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtfidf_train_channel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_Xtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'channel'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtfidf_train_channel_topics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtfidf_Xtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'channel'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_Xtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topics'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_Xtrain' is not defined"
     ]
    }
   ],
   "source": [
    "# TF-IDF Fit\n",
    "Xtrain, Xval, y_train, y_val = train_test_split(train_df, train_df['label'], test_size=0.3, random_state=30)\n",
    "doc_words = pd.concat([Xtrain['article'], Xtrain['h1'], Xtrain['title'], Xtrain['h2']])\n",
    "tfidf_train = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "tfidf_train_channel = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "tfidf_train_channel_topics = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "tfidf_train.fit(doc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\bs4\\__init__.py:343: MarkupResemblesLocatorWarning: \"music\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(preprocessor=<function preprocessor at 0x0000025D4F50DD38>,\n",
       "                tokenizer=<function tokenizer_stem_nostop at 0x0000025D38E83F78>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_channel.fit(Xtrain['channel'])\n",
    "tfidf_train_channel_topics.fit(pd.concat([Xtrain['channel'], Xtrain['topics']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Transform\n",
    "text_feature = ['article', 'h1', 'h2']\n",
    "training_sparse = []\n",
    "val_sparse = []\n",
    "for i in text_feature:\n",
    "    training_sparse.append(tfidf_train.transform(Xtrain[i]))\n",
    "    val_sparse.append(tfidf_train.transform(Xval[i]))\n",
    "training_sparse.append(tfidf_train_channel.transform(Xtrain['channel']))\n",
    "val_sparse.append(tfidf_train_channel.transform(Xval['channel']))\n",
    "training_sparse.append(tfidf_train_channel_topics.transform(Xtrain['channel']))\n",
    "val_sparse.append(tfidf_train_channel_topics.transform(Xval['channel']))\n",
    "training_sparse.append(tfidf_train_channel_topics.transform(Xtrain['topics']))\n",
    "val_sparse.append(tfidf_train_channel_topics.transform(Xval['topics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19350, 34)\n",
      "(19350, 9409)\n",
      "(19350, 9409)\n"
     ]
    }
   ],
   "source": [
    "for i in training_sparse:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "training_sparsematrix = sparse.hstack((training_sparse[0], training_sparse[1]))\n",
    "training_sparsematrix = sparse.hstack((training_sparsematrix, training_sparse[2]))\n",
    "training_sparsematrix2 = sparse.hstack((training_sparsematrix, training_sparse[4]))\n",
    "training_sparsematrix2 = sparse.hstack((training_sparsematrix2, training_sparse[5]))\n",
    "training_sparsematrix = sparse.hstack((training_sparsematrix, training_sparse[3]))\n",
    "training_sparse[0].shape\n",
    "training_sparsematrix.shape\n",
    "training_sparsematrix2.shape\n",
    "val_sparsematrix = sparse.hstack((val_sparse[0], val_sparse[1]))\n",
    "val_sparsematrix = sparse.hstack((val_sparsematrix, val_sparse[2]))\n",
    "val_sparsematrix2 = sparse.hstack((val_sparsematrix, val_sparse[4]))\n",
    "val_sparsematrix2 = sparse.hstack((val_sparsematrix2, val_sparse[5]))\n",
    "val_sparsematrix = sparse.hstack((val_sparsematrix, val_sparse[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5545791344548965\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag3 = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.6, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag3.fit(training_sparsematrix2, y_train)\n",
    "score = roc_auc_score(y_val, bag3.predict_proba(val_sparsematrix2)[:,1])\n",
    "print(score)\n",
    "pickle.dump(bag3, open('tfidfonly.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5503059972670918\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.8, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1)\n",
    "bag.fit(train_sparsematrix, y_train)\n",
    "score = roc_auc_score(y_val, bag.predict_proba(test_sparsematrix)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5539296613661283\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag2 = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.8, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag2.fit(train_sparsematrix2, y_train)\n",
    "score = roc_auc_score(y_val, bag2.predict_proba(test_sparsematrix2)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45096409, 0.36608476, 0.57281428, ..., 0.50563726, 0.3408287 ,\n",
       "       0.34089643])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.predict_proba(test_sparsematrix)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================== DROPPING ================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df_solve[['dayhour', 'author_post', 'links', 'popularity', 'article_len', 'topics', 'channel', 'author', 'label']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, X_train['label'], test_size=0.3, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5476343117331155\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipe1 = Pipeline([('author', AuthorTransFormer(gate = 2)),\n",
    "                  ('channel', ChannelTransFormer(gate = 2)),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer(gate = 2)),\n",
    "                  ('topics', TopicsTransFormer(tps_list = topic_dic)),\n",
    "                  ('topics_author', AuthorTopicsPopTransFormer(tps_list = topic_dic)),\n",
    "                  ('std', StandardScaler()),\n",
    "                  (\"pca\", PCA()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.001,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipe1.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipe1.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5486324401332469\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipe2 = Pipeline([('author', AuthorTransFormer(gate = 2)),\n",
    "                  ('channel', ChannelTransFormer(gate = 2)),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer(gate = 2)),\n",
    "                  ('topics', TopicsTransFormer(tps_list = topic_dic)),\n",
    "                  ('topics_author', AuthorTopicsPopTransFormer(tps_list = topic_dic)),\n",
    "                  ('std', StandardScaler()),\n",
    "                  (\"pca\", PCA()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.001,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipe2.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipe2.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5383687866492542\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipeR = Pipeline([('author', AuthorTransFormer(gate = 2)),\n",
    "                  ('channel', ChannelTransFormer(gate = 2)),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer(gate = 2)),\n",
    "                  ('topics', TopicsTransFormer(tps_list = topic_dic)),\n",
    "                  ('topics_author', AuthorTopicsPopTransFormer(tps_list = topic_dic)),\n",
    "                  ('std', StandardScaler()),\n",
    "                  (\"pca\", PCA()),\n",
    "                  ('clf', RandomForestClassifier(n_estimators=200, random_state=30))])\n",
    "\n",
    "pipeR.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipeR.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5465968424899728\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipeNP = Pipeline([('author', AuthorTransFormer(gate = 2)),\n",
    "                  ('channel', ChannelTransFormer(gate = 2)),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer(gate = 2)),\n",
    "                  ('topics', TopicsTransFormer(tps_list = topic_dic)),\n",
    "                  ('topics_author', AuthorTopicsPopTransFormer(tps_list = topic_dic)),\n",
    "                  ('std', StandardScaler()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.001,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipeNP.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipeNP.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================== SPARSE ================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeR = Pipeline([('author', AuthorTransFormer(gate = 2)),\n",
    "                  ('channel', ChannelTransFormer(gate = 2)),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer(gate = 2)),\n",
    "                  ('topics', TopicsTransFormer(tps_list = topic_dic)),\n",
    "                  ('topics_author', AuthorTopicsPopTransFormer(tps_list = topic_dic)),\n",
    "                  ('std', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sparsematrix_both = pipeR.fit_transform(X_train)\n",
    "training_sparsematrix_together = sparse.hstack((training_sparsematrix_both, training_sparsematrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.61663950e-01 -1.16004486e+00 -3.55345294e-01 ... -5.59961820e-01\n",
      "   4.19285454e-02 -5.64675354e-01]\n",
      " [ 1.21723403e+00  1.72861227e+00 -6.88063286e-03 ...  2.93439337e-14\n",
      "  -4.33509082e-01  2.35463275e+00]\n",
      " [-1.66804313e+00 -2.11660442e-01 -6.16693790e-01 ... -6.54841133e-01\n",
      "  -8.72606965e-02  1.88694479e-01]\n",
      " ...\n",
      " [ 2.38048596e-03  1.97275959e-01 -4.42461459e-01 ... -1.66082954e-01\n",
      "  -1.90506472e+00 -2.35076052e-01]\n",
      " [ 4.57950564e-01 -3.18444359e-02 -7.03809955e-01 ... -3.12235285e-01\n",
      "  -8.74975774e-01 -2.35076052e-01]\n",
      " [-1.97175652e+00 -7.51108460e-01 -6.16693790e-01 ...  2.93439337e-14\n",
      "  -1.37992134e+00 -3.76332896e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(training_sparsematrix_together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Founders of Flipboard Evan Doll and Mike McCue essentially met on a blind date at a coffee shop. They instantly knew they wanted to build something together, but they weren\\'t sure what.  On this episode of the Valley Girl Show, Doll shares that the duo tried quite a few things before settling on the right product. The initial idea came from trying to re-invent the web browser.  \"With startups you can always get rid of the product and build another one, but it\\'s harder to change the team,\" he says. Design is another important element that came into play. \"The iPad is what crystallized everything for us,\" Doll adds. It didn\\'t take them long to realize that it would enable people to peruse content in a magazine-like way.  With so much happening on social networks these days, Flipboard is one of the only content curation products that has clearly cut through the noise. It\\'s not just news, it\\'s personalized news. \"We try to bring relevancy to it,\" says Doll.  Doll taught the first ever class on iPhone app development at Stanford University. As an added bonus in this episode, Doll teaches us to code.  Jesse Draper is creator and host of The Valley Girl Show, through which she\\'s become a spokesperson for startups and helped pioneer the way of new media content distribution. Formerly a Nickelodeon star, Draper is now CEO of Valley Girl, where she oversees the show and runs technology blog Lalawag.com. More Video from the Valley Girl Show  Copious Is the Social Market Where Everyone Knows Your Name This Couple Owns the Online Bedding Space Stream Radio Stations From Around the World  Image courtesy of Valley Girl TV'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-c0a1e8886c98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_sparsematrix_both\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_sparsematrix_together\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_sparsematrix_both\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_sparsematrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    374\u001b[0m             \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m                 return last_step.fit(Xt, y,\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    696\u001b[0m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0;32m    697\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m                                 force_all_finite='allow-nan')\n\u001b[0m\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1780\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1781\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Founders of Flipboard Evan Doll and Mike McCue essentially met on a blind date at a coffee shop. They instantly knew they wanted to build something together, but they weren\\'t sure what.  On this episode of the Valley Girl Show, Doll shares that the duo tried quite a few things before settling on the right product. The initial idea came from trying to re-invent the web browser.  \"With startups you can always get rid of the product and build another one, but it\\'s harder to change the team,\" he says. Design is another important element that came into play. \"The iPad is what crystallized everything for us,\" Doll adds. It didn\\'t take them long to realize that it would enable people to peruse content in a magazine-like way.  With so much happening on social networks these days, Flipboard is one of the only content curation products that has clearly cut through the noise. It\\'s not just news, it\\'s personalized news. \"We try to bring relevancy to it,\" says Doll.  Doll taught the first ever class on iPhone app development at Stanford University. As an added bonus in this episode, Doll teaches us to code.  Jesse Draper is creator and host of The Valley Girl Show, through which she\\'s become a spokesperson for startups and helped pioneer the way of new media content distribution. Formerly a Nickelodeon star, Draper is now CEO of Valley Girl, where she oversees the show and runs technology blog Lalawag.com. More Video from the Valley Girl Show  Copious Is the Social Market Where Everyone Knows Your Name This Couple Owns the Online Bedding Space Stream Radio Stations From Around the World  Image courtesy of Valley Girl TV'"
     ]
    }
   ],
   "source": [
    "val_sparsematrix_both = pipeR.transform(X_val)\n",
    "val_sparsematrix_together = sparse.hstack((val_sparsematrix_both, val_sparsematrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sparsematrix_both = np.where(val_sparsematrix_both<0, abs(val_sparsematrix_both), val_sparsematrix_both)\n",
    "val_sparsematrix_together = sparse.hstack((val_sparsematrix_both, val_sparsematrix))\n",
    "train_sparsematrix_both = np.where(train_sparsematrix_both<0, abs(train_sparsematrix_both), train_sparsematrix_both)\n",
    "train_sparsematrix_together = sparse.hstack((train_sparsematrix_both, train_sparsematrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55901877186734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag3 = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.6, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag3.fit(train_sparsematrix_together, y_train)\n",
    "score = roc_auc_score(y_val, bag3.predict_proba(test_sparsematrix_together)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sparsematrix_both = pipeR.fit_transform(X_train)\n",
    "val_sparsematrix_both = pipeR.transform(X_val)\n",
    "val_sparsematrix_both = np.where(val_sparsematrix_both<0, 0, val_sparsematrix_both)\n",
    "val_sparsematrix_together = sparse.hstack((val_sparsematrix_both, val_sparsematrix))\n",
    "training_sparsematrix_both = np.where(training_sparsematrix_both<0, 0, training_sparsematrix_both)\n",
    "training_sparsematrix_together = sparse.hstack((training_sparsematrix_both, training_sparsematrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558858731455681\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag4 = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.6, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag4.fit(training_sparsematrix_together, y_train)\n",
    "score = roc_auc_score(y_val, bag4.predict_proba(val_sparsematrix_together)[:,1])\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(bag4, open('sparseMNB.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6228782133204835\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open('sparseMNB.pkl', 'rb'))\n",
    "score = roc_auc_score(y_val, loaded_model.predict_proba(test_sparsematrix_together)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6214532716915864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag4 = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.8, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag4.fit(train_sparsematrix_together, y_train)\n",
    "score = roc_auc_score(y_val, bag4.predict_proba(test_sparsematrix_together)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\bs4\\__init__.py:343: MarkupResemblesLocatorWarning: \"music\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(preprocessor=<function preprocessor at 0x000001C26A596C18>,\n",
       "                tokenizer=<function tokenizer_stem_nostop at 0x000001C253F2DEE8>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df_solve[['dayhour', 'author_post', 'links', 'popularity', 'article_len', 'topics', 'channel', 'author', 'label']]\n",
    "y_train = train_df_solve['label']\n",
    "# TF-IDF Fit\n",
    "tfidf_Xtrain, y_train = train_df, train_df['label']\n",
    "doc_words = pd.concat([tfidf_Xtrain['article'], tfidf_Xtrain['h1'], tfidf_Xtrain['title'], tfidf_Xtrain['h2']])\n",
    "tfidf = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "tfidf_channel = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "tfidf_channel_topics = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "tfidf.fit(doc_words)\n",
    "tfidf_channel.fit(tfidf_Xtrain['channel'])\n",
    "tfidf_channel_topics.fit(pd.concat([tfidf_Xtrain['channel'], tfidf_Xtrain['topics']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_solve = AuthorProblem(test_df, 'test')\n",
    "tfidf_Xval = test_df_solve[['dayhour', 'author_post', 'links', 'popularity', 'article_len', 'topics', 'channel', 'author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Transform\n",
    "text_feature = ['article', 'h1', 'h2']\n",
    "train_sparse = []\n",
    "test_sparse = []\n",
    "for i in text_feature:\n",
    "    train_sparse.append(tfidf.transform(tfidf_Xtrain[i]))\n",
    "    test_sparse.append(tfidf.transform(test_df[i]))\n",
    "train_sparse.append(tfidf_channel.transform(tfidf_Xtrain['channel']))\n",
    "test_sparse.append(tfidf_channel.transform(test_df['channel']))\n",
    "train_sparse.append(tfidf_channel_topics.transform(tfidf_Xtrain['channel']))\n",
    "test_sparse.append(tfidf_channel_topics.transform(test_df['channel']))\n",
    "train_sparse.append(tfidf_channel_topics.transform(tfidf_Xtrain['topics']))\n",
    "test_sparse.append(tfidf_channel_topics.transform(test_df['topics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "train_sparsematrix = sparse.hstack((train_sparse[0], train_sparse[1]))\n",
    "train_sparsematrix = sparse.hstack((train_sparsematrix, train_sparse[2]))\n",
    "train_sparsematrix2 = sparse.hstack((train_sparsematrix, train_sparse[4]))\n",
    "train_sparsematrix2 = sparse.hstack((train_sparsematrix2, train_sparse[5]))\n",
    "train_sparsematrix = sparse.hstack((train_sparsematrix, train_sparse[3]))\n",
    "train_sparse[0].shape\n",
    "train_sparsematrix.shape\n",
    "train_sparsematrix2.shape\n",
    "test_sparsematrix = sparse.hstack((test_sparse[0], test_sparse[1]))\n",
    "test_sparsematrix = sparse.hstack((test_sparsematrix, test_sparse[2]))\n",
    "test_sparsematrix2 = sparse.hstack((test_sparsematrix, test_sparse[4]))\n",
    "test_sparsematrix2 = sparse.hstack((test_sparsematrix2, test_sparse[5]))\n",
    "test_sparsematrix = sparse.hstack((test_sparsematrix, test_sparse[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27643, 135449)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sparse[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag_final = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.6, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag_final.fit(train_sparsematrix2, y_train)\n",
    "pickle.dump(bag_final, open('TFIDF_final.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipe_final = Pipeline([('author', AuthorTransFormer(gate = 2)),\n",
    "                  ('channel', ChannelTransFormer(gate = 2)),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer(gate = 2)),\n",
    "                  ('topics', TopicsTransFormer(tps_list = topic_dic)),\n",
    "                  ('topics_author', AuthorTopicsPopTransFormer(tps_list = topic_dic)),\n",
    "                  ('std', StandardScaler()),\n",
    "                  (\"pca\", PCA()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.001,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipe_final.fit(X_train, y_train)\n",
    "pickle.dump(pipe_final, open('Features_final.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 348152 and input n_features is 406381.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-77c9cbd6326d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpb_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mP_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_Xval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mP_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbag_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sparsematrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_Xval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    710\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is {0} and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m                              \u001b[1;34m\"input n_features is {1}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                              \"\".format(self.n_features_, X.shape[1]))\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;31m# Parallel loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 348152 and input n_features is 406381."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#pipe_final = pickle.load(open('featuresonly.pkl', 'rb'))\n",
    "#bag_final = pickle.load(open('TFIDFonly.pkl', 'rb'))\n",
    "pb_list = []\n",
    "P_1 = pipe_final.predict_proba(tfidf_Xval)[:,1]\n",
    "P_2 = bag_final.predict_proba(test_sparsematrix)[:,1]\n",
    "for d in range(len(tfidf_Xval)):\n",
    "    p1 = P_1[d]\n",
    "    p2 = P_2[d]\n",
    "    pb_list.append(p1*0.92 + p2*(1-0.92))\n",
    "    \n",
    "result = pb_list.round(1)\n",
    "print(result[:,1])\n",
    "submit_ans = pd.DataFrame()\n",
    "submit_ans['Id'] = test_df['id']\n",
    "submit_ans['Popularity'] = result[:,1]\n",
    "\n",
    "print(submit_ans)\n",
    "submit_ans.to_csv('./out.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "train_sparsematrix_both = pipeR.fit_transform(X_train)\n",
    "test_sparsematrix_both = pipeR.transform(tfidf_Xval)\n",
    "test_sparsematrix_both = np.where(test_sparsematrix_both<0, 0, test_sparsematrix_both)\n",
    "test_sparsematrix_together = sparse.hstack((test_sparsematrix_both, test_sparsematrix))\n",
    "train_sparsematrix_both = np.where(train_sparsematrix_both<0, 0, train_sparsematrix_both)\n",
    "train_sparsematrix_together = sparse.hstack((train_sparsematrix_both, train_sparsematrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=MultinomialNB(), max_samples=0.6,\n",
       "                  n_estimators=500, n_jobs=1, random_state=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag5 = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.6, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag5.fit(train_sparsematrix_together, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.5 0.3 ... 0.5 0.3 0.3]\n",
      "          Id  Popularity\n",
      "0      27643         0.3\n",
      "1      27644         0.5\n",
      "2      27645         0.3\n",
      "3      27646         0.4\n",
      "4      27647         0.6\n",
      "...      ...         ...\n",
      "11842  39485         0.7\n",
      "11843  39486         0.6\n",
      "11844  39487         0.5\n",
      "11845  39488         0.3\n",
      "11846  39489         0.3\n",
      "\n",
      "[11847 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "result = bag5.predict_proba(test_sparsematrix_together)\n",
    "result = result.round(1)\n",
    "print(result[:,1])\n",
    "submit_ans = pd.DataFrame()\n",
    "submit_ans['Id'] = test_df['id']\n",
    "submit_ans['Popularity'] = result[:,1]\n",
    "\n",
    "print(submit_ans)\n",
    "submit_ans.to_csv('./out.csv', index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================== ENSEMBLE ================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembleCV(model1, model2, alpha, val_df, X_val_other, X_val_text):\n",
    "    max = 0\n",
    "    maxpick = 0\n",
    "    l = []\n",
    "    for a in alpha:\n",
    "        pb_list = []\n",
    "        P_1 = model1.predict_proba(X_val_other)[:,1]\n",
    "        P_2 = model2.predict_proba(X_val_text)[:,1]\n",
    "        for d in range(len(val_df)):\n",
    "            p1 = P_1[d]\n",
    "            p2 = P_2[d]\n",
    "            pb_list.append(p1*a + p2*(1-a))\n",
    "        score = roc_auc_score(val_df, pb_list)\n",
    "        print(score)\n",
    "        if(score > max): \n",
    "            max = score \n",
    "            maxpick = a\n",
    "    return(max, maxpick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5545791344548965\n",
      "0.554755120711208\n",
      "0.5549559568787115\n",
      "0.5552092863012391\n",
      "0.555539784300443\n",
      "0.5559883630251945\n",
      "0.5565815600855721\n",
      "0.5574036440328787\n",
      "0.558668021481497\n",
      "0.560191024235356\n",
      "(0.560191024235356, 0.9)\n"
     ]
    }
   ],
   "source": [
    "pipe1 = pickle.load(open('featuresonly.pkl', 'rb'))\n",
    "ans = ensembleCV(pipe1, bag3, [(i / 100) for i in range(0, 100, 10)], y_val, X_val, val_sparsematrix2)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5594287663037532\n",
      "0.5596202910291238\n",
      "0.5597929600841755\n",
      "0.559986579884026\n",
      "0.5601217703844927\n",
      "0.560191024235356\n",
      "0.5602557969546929\n",
      "0.560321326228703\n",
      "0.5602285027899409\n",
      "0.5600164346953646\n"
     ]
    }
   ],
   "source": [
    "pipe1 = pickle.load(open('featuresonly.pkl', 'rb'))\n",
    "ans = ensembleCV(pipe1, bag3, [(i / 100) for i in range(85, 95, 1)], y_val, X_val, val_sparsematrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.92"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
