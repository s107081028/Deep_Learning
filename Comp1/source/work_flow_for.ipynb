{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, sys\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset_dir = './train/'\n",
    "test_dataset_dir = './test/'\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = None\n",
    "test_df = None\n",
    "train_files = listdir(train_dataset_dir)\n",
    "test_files = listdir(test_dataset_dir)\n",
    "\n",
    "train_df = pd.read_csv(train_dataset_dir + 'title_len.csv')\n",
    "test_df = pd.read_csv(test_dataset_dir + 'title_len.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in train_files:\n",
    "        s = f[:-4]\n",
    "        temp_df = pd.read_csv(train_dataset_dir + s + '.csv', lineterminator='\\n')\n",
    "        train_df[s] = temp_df[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in test_files:\n",
    "        s = f[:-4]\n",
    "        temp_df = pd.read_csv(test_dataset_dir + s + '.csv', lineterminator='\\n')\n",
    "        test_df[s] = temp_df[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解釋一下下面兩個transformer，可以當成standardScalar的感覺，call fit會把丟入的trainset去算dictionary跟mean(就跟SC的fit一樣，要先看train的平均一樣)，Call transform會把input transform成有popularity的datafram，所以可以放進pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5):\n",
    "        self.author_pop_dict = {}\n",
    "        self.author_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        \n",
    "    def get_author_pop(self, author_arr):\n",
    "        total_mean = []\n",
    "        for i in author_arr:\n",
    "            if i in self.author_pop_dict.keys():\n",
    "                total_mean.append(self.author_pop_dict[i])\n",
    "            else:\n",
    "                total_mean.append(self.author_pop_mean)\n",
    "        return total_mean\n",
    "\n",
    "    def get_train_author_pop(self, author_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for author, label in zip(author_arr, label_arr):\n",
    "            if author in popularity.keys():\n",
    "                popularity[author].append(int(label))\n",
    "            else:\n",
    "                popularity[author] = [int(label)]\n",
    "\n",
    "        delete_keys = []\n",
    "        for i in popularity.keys():\n",
    "            if(len(popularity[i]) < self.gate):\n",
    "                delete_keys.append(i)\n",
    "            else:\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "                \n",
    "        for j in delete_keys:\n",
    "            popularity.pop(j, None)\n",
    "        total_mean = []\n",
    "        cnt = 0\n",
    "        for i in author_arr:\n",
    "            if i in popularity.keys():\n",
    "                total_mean.append(popularity[i])\n",
    "            else:\n",
    "                cnt = cnt + 1\n",
    "                total_mean.append(-1)\n",
    "\n",
    "        mean = (sum(total_mean) + cnt)/ (len(total_mean) - cnt)\n",
    "        return popularity, mean\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.author_pop_dict, self.author_pop_mean = self.get_train_author_pop(X['author'], X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['popularity'] = self.get_author_pop(X['author'])\n",
    "        X= X.drop(['author'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicsTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5):\n",
    "        self.topics_pop_dict = {}\n",
    "        self.topics_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        \n",
    "    def get_topics_pop(self,topics_arr):\n",
    "        total_mean = []\n",
    "        for topics in topics_arr :\n",
    "            tps_val = -1\n",
    "            choose_tps = ''\n",
    "            for tps in topics.split():\n",
    "                if tps in self.topics_pop_dict.keys():\n",
    "                    if tps_val < self.topics_pop_dict[tps]:\n",
    "                        tps_val = self.topics_pop_dict[tps]\n",
    "                        choose_tps = tps\n",
    "            if topics == '' or choose_tps == '':\n",
    "                total_mean.append(self.topics_pop_mean)\n",
    "            else:\n",
    "                total_mean.append(self.topics_pop_dict[choose_tps])\n",
    "        return total_mean\n",
    "\n",
    "    def get_train_topics_pop(self, topics_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for topics, label in zip(topics_arr, label_arr):\n",
    "            for tps in topics.split():\n",
    "                if tps in popularity.keys():\n",
    "                    popularity[tps].append(int(label))\n",
    "                else:\n",
    "                    popularity[tps] = [int(label)]\n",
    "\n",
    "        delete_keys = []\n",
    "        for i in popularity.keys():\n",
    "            if len(popularity[i]) < self.gate:\n",
    "                delete_keys.append(i)\n",
    "            else:\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "        for j in delete_keys:\n",
    "            popularity.pop(j, None)\n",
    "\n",
    "        total_mean = []\n",
    "        cnt_non_tps = 0\n",
    "        for topics in topics_arr:\n",
    "            tps_val = -1\n",
    "            choose_tps = ''\n",
    "            for tps in topics.split():\n",
    "                if tps not in popularity.keys():\n",
    "                    continue\n",
    "                if tps_val < popularity[tps]:\n",
    "                    tps_val = popularity[tps]\n",
    "                    choose_tps = tps\n",
    "            if topics == '' or choose_tps == '':\n",
    "                cnt_non_tps += 1\n",
    "                total_mean.append(-1)\n",
    "            else:\n",
    "                total_mean.append(popularity[choose_tps])\n",
    "\n",
    "        mean = (sum(total_mean) + cnt_non_tps) / (len(total_mean) - cnt_non_tps)\n",
    "        return popularity, mean\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.topics_pop_dict, self.topics_pop_mean = self.get_train_topics_pop(X['topics'].fillna(\"\"), X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['topics_pop'] = self.get_topics_pop(X['topics'].fillna(\"\"))\n",
    "        if 'label' in X.columns:\n",
    "            X= X.drop(['topics', 'label'], axis=1)\n",
    "        else:\n",
    "            X= X.drop(['topics'], axis=1)\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5):\n",
    "        self.author_pop_dict = {}\n",
    "        self.author_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        \n",
    "    def get_author_pop(self, author_arr):\n",
    "        total_mean = []\n",
    "        for i in author_arr:\n",
    "            if i in self.author_pop_dict.keys():\n",
    "                total_mean.append(self.author_pop_dict[i])\n",
    "            else:\n",
    "                total_mean.append(self.author_pop_mean)\n",
    "        return total_mean\n",
    "\n",
    "    def get_train_author_pop(self, author_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for author, label in zip(author_arr, label_arr):\n",
    "            if author in popularity.keys():\n",
    "                popularity[author].append(int(label))\n",
    "            else:\n",
    "                popularity[author] = [int(label)]\n",
    "\n",
    "        delete_keys = []\n",
    "        for i in popularity.keys():\n",
    "            if(len(popularity[i]) < self.gate):\n",
    "                delete_keys.append(i)\n",
    "            else:\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "                \n",
    "        for j in delete_keys:\n",
    "            popularity.pop(j, None)\n",
    "        total_mean = []\n",
    "        cnt = 0\n",
    "        for i in author_arr:\n",
    "            if i in popularity.keys():\n",
    "                total_mean.append(popularity[i])\n",
    "            else:\n",
    "                cnt = cnt + 1\n",
    "                total_mean.append(-1)\n",
    "\n",
    "        mean = (sum(total_mean) + cnt)/ (len(total_mean) - cnt)\n",
    "        return popularity, mean\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.author_pop_dict, self.author_pop_mean = self.get_train_author_pop(X['channel'], X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['channel_pop'] = self.get_author_pop(X['channel'])\n",
    "        X= X.drop(['channel'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，authortrans一定要擺在topic的前面，topics一定要擺在SC的前面(因為中間輸出會drop 有文字的colums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe1 = Pipeline([('author', AuthorTransFormer()),\n",
    "                  ('channel', ChannelTransFormer()),\n",
    "                  ('topics', TopicsTransFormer()),\n",
    "                  ('std', StandardScaler()),\n",
    "#                   ('pca', PCA()),\n",
    "                  ('clf', LogisticRegression(C=0.0001, random_state=17, n_jobs=4))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意! 選擇需要的feature，但是label、topics、author一定要放!(除非沒放topic跟authortransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title_len', 'h2', 'dayhour', 'h1', 'author_post', 'channel',\n",
      "       'author', 'label', 'topics', 'nn_percentage', 'title', 'cnt_iframe',\n",
      "       'cnt_img', 'rb_percentage', 'topics_pop_5', 'jj_percentage', 'weekday',\n",
      "       'links', 'topics_pop', 'topics_pop_10', 'topics_pop_25',\n",
      "       'topics_pop_15', 'cnt_vdo', 'article', 'popularity', 'article_len',\n",
      "       'topics_pop_20', 'vb_percentage'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[['dayhour', 'author_post', 'nn_percentage', 'cnt_iframe','cnt_img','rb_percentage', 'jj_percentage','links', 'weekday', 'popularity', 'cnt_vdo', 'article_len', 'vb_percentage', 'topics', 'author', 'label', 'channel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, train_df['label'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('author', AuthorTransFormer()),\n",
       "                ('channel', ChannelTransFormer()),\n",
       "                ('topics', TopicsTransFormer()), ('std', StandardScaler()),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=0.0001, n_jobs=4, random_state=17))])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipe1.predict_proba(X_val)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5452264226672794\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
