{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, sys\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset_dir = './train/'\n",
    "test_dataset_dir = './test/'\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = None\n",
    "test_df = None\n",
    "train_files = listdir(train_dataset_dir)\n",
    "test_files = listdir(test_dataset_dir)\n",
    "\n",
    "train_df = pd.read_csv(train_dataset_dir + 'title_len.csv')\n",
    "test_df = pd.read_csv(test_dataset_dir + 'title_len.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in train_files:\n",
    "        s = f[:-4]\n",
    "        temp_df = pd.read_csv(train_dataset_dir + s + '.csv', lineterminator='\\n')\n",
    "        train_df[s] = temp_df[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in test_files:\n",
    "        s = f[:-4]\n",
    "        temp_df = pd.read_csv(test_dataset_dir + s + '.csv', lineterminator='\\n')\n",
    "        test_df[s] = temp_df[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解釋一下下面兩個transformer，可以當成standardScalar的感覺，call fit會把丟入的trainset去算dictionary跟mean(就跟SC的fit一樣，要先看train的平均一樣)，Call transform會把input transform成有popularity的datafram，所以可以放進pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5):\n",
    "        self.author_pop_dict = {}\n",
    "        self.author_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        \n",
    "    def get_author_pop(self, author_arr):\n",
    "        total_mean = []\n",
    "        for i in author_arr:\n",
    "            if i in self.author_pop_dict.keys():\n",
    "                total_mean.append(self.author_pop_dict[i])\n",
    "            else:\n",
    "                total_mean.append(self.author_pop_mean)\n",
    "        return total_mean\n",
    "\n",
    "    def get_train_author_pop(self, author_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for author, label in zip(author_arr, label_arr):\n",
    "            if author in popularity.keys():\n",
    "                popularity[author].append(int(label))\n",
    "            else:\n",
    "                popularity[author] = [int(label)]\n",
    "\n",
    "        delete_keys = []\n",
    "        for i in popularity.keys():\n",
    "            if(len(popularity[i]) < self.gate):\n",
    "                delete_keys.append(i)\n",
    "            else:\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "                \n",
    "        for j in delete_keys:\n",
    "            popularity.pop(j, None)\n",
    "        total_mean = []\n",
    "        cnt = 0\n",
    "        for i in author_arr:\n",
    "            if i in popularity.keys():\n",
    "                total_mean.append(popularity[i])\n",
    "            else:\n",
    "                cnt = cnt + 1\n",
    "                total_mean.append(-1)\n",
    "\n",
    "        mean = (sum(total_mean) + cnt)/ (len(total_mean) - cnt)\n",
    "        return popularity, mean\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.author_pop_dict, self.author_pop_mean = self.get_train_author_pop(X['author'], X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['popularity'] = self.get_author_pop(X['author'])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5):\n",
    "        self.author_pop_dict = {}\n",
    "        self.author_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        \n",
    "    def get_author_pop(self, author_arr):\n",
    "        total_mean = []\n",
    "        for i in author_arr:\n",
    "            if i in self.author_pop_dict.keys():\n",
    "                total_mean.append(self.author_pop_dict[i])\n",
    "            else:\n",
    "                total_mean.append(self.author_pop_mean)\n",
    "        return total_mean\n",
    "\n",
    "    def get_train_author_pop(self, author_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for author, label in zip(author_arr, label_arr):\n",
    "            if author in popularity.keys():\n",
    "                popularity[author].append(int(label))\n",
    "            else:\n",
    "                popularity[author] = [int(label)]\n",
    "\n",
    "        delete_keys = []\n",
    "        for i in popularity.keys():\n",
    "            if(len(popularity[i]) < self.gate):\n",
    "                delete_keys.append(i)\n",
    "            else:\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "                \n",
    "        for j in delete_keys:\n",
    "            popularity.pop(j, None)\n",
    "        total_mean = []\n",
    "        cnt = 0\n",
    "        for i in author_arr:\n",
    "            if i in popularity.keys():\n",
    "                total_mean.append(popularity[i])\n",
    "            else:\n",
    "                cnt = cnt + 1\n",
    "                total_mean.append(-1)\n",
    "\n",
    "        mean = (sum(total_mean) + cnt)/ (len(total_mean) - cnt)\n",
    "        return popularity, mean\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.author_pop_dict, self.author_pop_mean = self.get_train_author_pop(X['channel'], X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['channel_pop'] = self.get_author_pop(X['channel'])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorTopicsPopTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5, tps_list = []):\n",
    "        self.author_pop_dict = {}\n",
    "        self.author_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        self.df = None\n",
    "        self.tps_list = tps_list\n",
    "        \n",
    "    def get_author_channel_pop(self, channel_arr, author_arr, author_pop_arr):\n",
    "        arr = []\n",
    "        len_arr = []\n",
    "        for author, channel, author_pop in zip(author_arr, channel_arr, author_pop_arr):\n",
    "            author_channel_df = self.df.loc[self.df['author'] == author]\n",
    "            author_channel_df = author_channel_df.loc[author_channel_df['choose_tps'] == channel] \n",
    "            if (len(author_channel_df) < self.gate) or (channel not in self.tps_list):\n",
    "                arr.append(author_pop)\n",
    "                len_arr.append(len(author_channel_df))\n",
    "            else:\n",
    "                positive = len(author_channel_df.loc[author_channel_df['label'] == 1])\n",
    "                total = len(author_channel_df)\n",
    "                arr.append(positive/total)\n",
    "                len_arr.append(total)\n",
    "        return arr, len_arr\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        self.df = X.loc[:, ['author', 'choose_tps', 'label']] \n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['author_tps_pop'],  X['author_tps_count'] = self.get_author_channel_pop(X['choose_tps'],  X['author'],  X['popularity'])\n",
    "        if 'label' in X.columns:\n",
    "            X= X.drop(['author','choose_tps', 'label'], axis=1)\n",
    "        else:\n",
    "            X= X.drop(['author', 'choose_tps'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicsTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5, tps_list = []):\n",
    "        self.topics_pop_dict = {}\n",
    "        self.topics_pop_mean = 0\n",
    "        self.gate = 5\n",
    "        self.tps_list = tps_list\n",
    "        \n",
    "    def get_topics_pop(self,topics_arr):\n",
    "        total_mean = []\n",
    "        choose_topic = []\n",
    "        for topics in topics_arr :\n",
    "            tps_val = -1\n",
    "            choose_tps = ''\n",
    "            for tps in topics.split():\n",
    "                if tps in self.topics_pop_dict.keys():\n",
    "                    if tps_val < self.topics_pop_dict[tps]:\n",
    "                        tps_val = self.topics_pop_dict[tps]\n",
    "                        choose_tps = tps\n",
    "            if topics == '' or choose_tps == '':\n",
    "                total_mean.append(self.topics_pop_mean)\n",
    "                choose_topic.append('NONE')\n",
    "            else:\n",
    "                total_mean.append(self.topics_pop_dict[choose_tps])\n",
    "                choose_topic.append(choose_tps)\n",
    "        return total_mean, choose_topic\n",
    "\n",
    "    def get_train_topics_pop(self, topics_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for i in self.tps_list:\n",
    "            popularity[i] = []\n",
    "        for topics, label in zip(topics_arr, label_arr):\n",
    "            for tps in topics.split():\n",
    "                if tps in popularity.keys():\n",
    "                    popularity[tps].append(int(label))\n",
    "        for i in popularity.keys():\n",
    "            if len(popularity[i]) != 0:\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "            else: \n",
    "                print(i)\n",
    "        \n",
    "        total_mean = []\n",
    "        cnt_non_tps = 0\n",
    "        for topics in topics_arr:\n",
    "            tps_val = -1\n",
    "            choose_tps = ''\n",
    "            for tps in topics.split():\n",
    "                if tps not in popularity.keys():\n",
    "                    continue\n",
    "                if tps_val < popularity[tps]:\n",
    "                    tps_val = popularity[tps]\n",
    "                    choose_tps = tps\n",
    "            if topics == '' or choose_tps == '':\n",
    "                cnt_non_tps += 1\n",
    "                total_mean.append(-1)\n",
    "            else:\n",
    "                total_mean.append(popularity[choose_tps])\n",
    "\n",
    "        mean = (sum(total_mean) + cnt_non_tps) / (len(total_mean) - cnt_non_tps)\n",
    "        return popularity, mean\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.topics_pop_dict, self.topics_pop_mean = self.get_train_topics_pop(X['topics'].fillna(\"\"), X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['topics_pop'] , X['choose_tps'] = self.get_topics_pop(X['topics'].fillna(\"\"))\n",
    "        X= X.drop(['topics'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，authortrans一定要擺在topic的前面，topics一定要擺在SC的前面(因為中間輸出會drop 有文字的colums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorChannelPopTransFormer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, gate = 5):\n",
    "        self.author_channel_pop_dict = {}\n",
    "        self.author_channel_pop_mean = 0\n",
    "        self.author_channel_cnt_dict = {}\n",
    "        self.author_channel_cnt_mean = 0\n",
    "        self.gate = 5\n",
    "        self.df = None\n",
    "        \n",
    "    def get_author_channel_pop(self, channel_arr, author_arr):\n",
    "        arr = []\n",
    "        len_arr = []\n",
    "        for author, channel in zip(author_arr, channel_arr):\n",
    "            if (author, channel) not in self.author_channel_pop_dict.keys():\n",
    "                arr.append(self.author_channel_pop_mean)\n",
    "                len_arr.append(self.author_channel_cnt_mean)\n",
    "            else:\n",
    "                arr.append(self.author_channel_pop_dict[(author, channel)])\n",
    "                len_arr.append(self.author_channel_cnt_dict[(author, channel)])\n",
    "        return arr, len_arr\n",
    "    \n",
    "    def get_train_author_channel_pop(self, channel_arr, author_arr, label_arr):\n",
    "        popularity = {}\n",
    "        for author, channel, label in zip(author_arr, channel_arr, label_arr):\n",
    "            if (author, channel) in popularity.keys():\n",
    "                popularity[(author, channel)].append(int(label))\n",
    "            else:\n",
    "                popularity[(author, channel)] = [int(label)]\n",
    "            \n",
    "\n",
    "        delete_keys = []\n",
    "        cnt_mean = []\n",
    "        cnt_dict = {}\n",
    "        for i in popularity.keys():\n",
    "            if len(popularity[i]) < self.gate:\n",
    "                delete_keys.append(i)\n",
    "            else:\n",
    "                cnt_dict[i] = len(popularity[i])\n",
    "                cnt_mean.append(len(popularity[i]))\n",
    "                popularity[i] = sum(popularity[i]) / len(popularity[i])\n",
    "        for j in delete_keys:\n",
    "            popularity.pop(j, None)\n",
    "\n",
    "        total_mean = []\n",
    "        cnt_non_author_channel = 0\n",
    "        mean_cnt = 0\n",
    "        for author, channel in zip(author_arr, channel_arr):\n",
    "            if (author, channel) in popularity.keys():\n",
    "                total_mean.append(popularity[(author, channel)])\n",
    "            else:\n",
    "                cnt_non_author_channel += 1\n",
    "                total_mean.append(-1)\n",
    "        \n",
    "        mean = (sum(total_mean) + cnt_non_author_channel) / (len(total_mean) - cnt_non_author_channel)\n",
    "        mean_cnt = sum(cnt_mean) / len(cnt_mean)\n",
    "        return popularity, mean, cnt_dict, mean_cnt\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        self.author_channel_pop_dict, self.author_channel_pop_mean, self.author_channel_cnt_dict, self.author_channel_cnt_mean = self.get_train_author_channel_pop(X['channel'], X['author'], X['label'])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['author_channel_pop'],  X['author_channel_count'] = self.get_author_channel_pop(X['channel'], X['author'])\n",
    "        X= X.drop(['channel'], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dic  = ['apps', 'facebook', 'design', 'photography', 'youtube', 'google', 'twitter', 'family', 'television', 'gallery', 'movies', 'ukraine', 'search', 'thrones', 'contributor', 'glass', 'humor', 'space', 'health', 'netflix', 'wars', 'east', 'linkedin', 'climate', 'cup', 'memes', 'reddit', 'winter', 'smartphone', 'vine', 'spotify', 'holiday', 'celebrities', 'yahoo', 'gmail', 'snapchat', 'diy', 'videos', 'launchpad', 'choice', 'earth', '2014', '360', 'chrome', 'shopping', 'california', 'tablet', 'small', 'ebay', 'game', 'house', 'instagram', 'holidays', 'web', 'snow', 'streaming', 'cyrus', 'late', 'lego', 'week', 'doctor', 'motorola', 'sxsw', 'reviews', 'parenting', 'nostalgia', 'rift', 'infographics', 'pandora', \"valentine's\", 'freedom', 'emmys', 'doodle', 'roundup', 'reality', 'angry', 'healthcare', 'snowden', 'gifs', 'valley', 'digital', 'beer', 'mad', 'motion', 'marathon', 'security', 'cover', 'journalism', 'features', 'bill', 'gates', 'parody', 'selfie', 'sunday', 'brands', 'beauty', 'star', 'toys', 'iphone', 'interviews', 'cases', 'solutions', 'america', 'augmented', 'human', 'now', 'recent', 'ibm', 'gift', 'crash', 'tourism', 'mars', 'electronics', 'green', 'williams', 'life', 'kickstarter', 'vimeo', 'man', 'academy', 'death', 'fashion', 'texting', 'tips', 'resume', 'xbox', 'storage', 'touch', 'kids', 'olympic', 'facts', 'siri', 'spring', 'at&t', 'hbo', 'forum', 'kingdom', 'nest', 'south', 'arrested', 'iwatch', 'los', 'anonymous', 'germany', 'opinion', 'use', 'scrivan', 'european', 'email', 'travel', 'venezuela', 'smartwatch', 'moon', 'international', 'police', 'jerry', 'costumes', 'history', 'psychology', 'congress', 'cyberbullying', 'pope', 'awards', 'lawsuit', 'hacks', 'study', 'drone', 'smart', 'easter', 'ferguson', 'ted', 'width', 'cbs', 'dna', 'management', 'teens', 'olympics', 'safety', 'stem', 'yelp', 'buzzwords', 'superheroes', 'legos', 'brazil', 'cable', 'disease', 'campaign', 'paris', 'perry', 'battery', 'crafts', 'gold', 'new', 'printers', 'iron', 'bad', 'pew', 'crime', 'news', 'online', 'korea', 'art', 'extreme', 'waze', 'fox', 'dogs', 'texas', 'retina', 'seinfeld', 'hangouts', 's5', 'cute', 'box', 'future', 'wikipedia', 'team', 'animated', 'jose', 'mashups', 'living', 'ocean', 'productivity', 'true', 'voice', 'speech', 'sam', 'fov', 'password', 'thanksgiving', 'seo', 'beat', 'deaths', 'internet', 'hashtags', 'conan', 'franco', 'smartwatches', 'philippines', 'graph', 'shazam', 'recalls', 'cameras', 'hubble', 'content', 'law', 'muppets', 'wireless', 'alibaba', 'fiber', 'browser', 'metrics', 'zelda', 'oliver', 'students', 'station', 'watson', 'printing', 'rss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipe1 = Pipeline([('author', AuthorTransFormer()),\n",
    "                  ('channel', ChannelTransFormer()),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer()),\n",
    "                  ('topics', TopicsTransFormer()),\n",
    "                  ('std', StandardScaler()),\n",
    "                  (\"pca\", PCA()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.015,objective=\"binary:logistic\", eval_metric=\"error\"))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意選擇需要的feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[['dayhour', 'author_post', 'nn_percentage', 'rb_percentage', 'jj_percentage', 'links', 'popularity', 'article_len', 'vb_percentage', 'topics', 'channel', 'author', 'label']]\n",
    "X_test = test_df[['dayhour', 'author_post', 'nn_percentage', 'rb_percentage', 'jj_percentage', 'links', 'popularity', 'article_len', 'vb_percentage', 'topics', 'channel', 'author']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, train_df['label'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5275715569098418\n"
     ]
    }
   ],
   "source": [
    "pipe1.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipe1.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.544862320638823\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipe1 = Pipeline([('author', AuthorTransFormer(gate = 2)),\n",
    "                  ('channel', ChannelTransFormer(gate = 2)),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer(gate = 2)),\n",
    "                  ('topics', TopicsTransFormer(tps_list = topic_dic)),\n",
    "                  ('topics_author', AuthorTopicsPopTransFormer(tps_list = topic_dic)),\n",
    "                  ('std', StandardScaler()),\n",
    "                  (\"pca\", PCA()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.001,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipe1.fit(X_t, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipe1.predict_proba(X_v)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5434609757786665\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipe1 = Pipeline([('author', AuthorTransFormer(gate = 2)),\n",
    "                  ('channel', ChannelTransFormer(gate = 2)),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer(gate = 2)),\n",
    "                  ('topics', TopicsTransFormer(tps_list = topic_dic)),\n",
    "                  ('topics_author', AuthorTopicsPopTransFormer(tps_list = topic_dic)),\n",
    "                  ('std', StandardScaler()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.001,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipe1.fit(X_t, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipe1.predict_proba(X_v)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = AuthorProblem(X_train, 'train')\n",
    "y_train = X_t['label']\n",
    "X_v = AuthorProblem(X_val, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9123"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stanl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def preprocessor(text):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['h2'] = train_df['h2'].fillna('')\n",
    "test_df['h2'] = test_df['h2'].fillna('')\n",
    "test_df['article'] = test_df['article'].fillna('')\n",
    "train_df['author'] = train_df['author'].fillna('')\n",
    "test_df['author'] = test_df['author'].fillna('')\n",
    "train_df['title'] = train_df['title'].fillna('')\n",
    "test_df['title'] = test_df['title'].fillna('')\n",
    "train_df['h1'] = train_df['h1'].fillna('') + train_df['author']\n",
    "test_df['h1'] = test_df['h1'].fillna('') + test_df['author']\n",
    "train_df['topics'] = train_df['topics'].fillna('')\n",
    "test_df['topics'] = test_df['topics'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\bs4\\__init__.py:343: MarkupResemblesLocatorWarning: \"music\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(preprocessor=<function preprocessor at 0x0000027700BB13A8>,\n",
       "                tokenizer=<function tokenizer_stem_nostop at 0x000002770C289A68>)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Fit\n",
    "tfidf_Xtrain, tfidf_Xval, y_train, y_val = train_test_split(train_df, train_df['label'], test_size=0.3, random_state=30)\n",
    "doc_words = pd.concat([tfidf_Xtrain['article'], tfidf_Xtrain['h1'], tfidf_Xtrain['title'], tfidf_Xtrain['h2']])\n",
    "tfidf = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "tfidf_channel = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "tfidf_channel_topics = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "tfidf.fit(doc_words)\n",
    "tfidf_channel.fit(tfidf_Xtrain['channel'])\n",
    "tfidf_channel_topics.fit(pd.concat([tfidf_Xtrain['channel'], tfidf_Xtrain['topics']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  title_len  article_len                        author  \\\n",
      "0          0          8          577               Clara Moskowitz   \n",
      "1          1         12          305              Christina Warren   \n",
      "2          2         12         1114                     Sam Laird   \n",
      "3          3          5          278                     Sam Laird   \n",
      "4          4         10         1370               Connor Finnegan   \n",
      "...      ...        ...          ...                           ...   \n",
      "27638  27638          9          303  Lorenzo FranceschiBicchierai   \n",
      "27639  27639          9          317                Adario Strange   \n",
      "27640  27640          8          170            Christine Erickson   \n",
      "27641  27641          8          430                Seth Fiegerman   \n",
      "27642  27642         10          472                  Megan Ranney   \n",
      "\n",
      "       author_post         channel  cnt_iframe  cnt_img  cnt_vdo  dayhour  \\\n",
      "0               18           world           0        2        0       15   \n",
      "1              399            tech           0        3        0       17   \n",
      "2             1004   entertainment          25       28        0       19   \n",
      "3             1004     watercooler          21       21        0        2   \n",
      "4               87   entertainment           1      103        0        3   \n",
      "...            ...             ...         ...      ...      ...      ...   \n",
      "27638          696           world           0        3        0       16   \n",
      "27639          377            tech           0        5        0        1   \n",
      "27640          263     watercooler           0       21        0       12   \n",
      "27641          923        business           0        5        0       20   \n",
      "27642           19  small-business           0        2        0       18   \n",
      "\n",
      "       ...                                              title  \\\n",
      "0      ...  NASA's Grand Challenge: Stop Asteroids From De...   \n",
      "1      ...  Google's New Open Source Patent Pledge: We Won...   \n",
      "2      ...  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
      "3      ...        Cameraperson Fails Deliver Slapstick Laughs   \n",
      "4      ...  NFL Star Helps Young Fan Prove Friendship With...   \n",
      "...    ...                                                ...   \n",
      "27638  ...  Chief of USAID Doesn't Know Who Created 'Cuban...   \n",
      "27639  ...  Photo of Samsung's Rumored Virtual Reality Hea...   \n",
      "27640  ...          14 Dogs That Frankly Cannot Take the Heat   \n",
      "27641  ...  Yahoo Earnings Beat Estimates, But Core Proble...   \n",
      "27642  ...  The winners of our #CurioCity contest tour Aus...   \n",
      "\n",
      "                                                  topics  topics_pop  \\\n",
      "0      asteroid asteroids challenge earth space u.s. ...    0.610000   \n",
      "1      patent-lawsuit theater apps and software googl...    1.000000   \n",
      "2      nfl espn entertainment nfl nfl draft sports te...    0.700000   \n",
      "3                youtube sports video videos watercooler    0.509849   \n",
      "4      nfl instagram entertainment instagram instagra...    0.513339   \n",
      "...                                                  ...         ...   \n",
      "27638             cuba internet freedom u.s. world usaid    0.666667   \n",
      "27639  oculus rift samsung apps and software dev & de...    0.615385   \n",
      "27640  summer food hot dogs humor photography waterco...    0.542493   \n",
      "27641          business marissa mayer media stocks yahoo    0.529240   \n",
      "27642  austin business curiocity small business startups    0.750000   \n",
      "\n",
      "       topics_pop_10  topics_pop_15  topics_pop_20 topics_pop_25 topics_pop_5  \\\n",
      "0           0.610000       0.610000       0.610000      0.610000     0.610000   \n",
      "1           0.615385       0.612903       0.612903      0.612903     0.615385   \n",
      "2           0.700000       0.501658       0.501658      0.501658     0.700000   \n",
      "3           0.509849       0.509849       0.509849      0.509849     0.509849   \n",
      "4           0.513339       0.513339       0.513339      0.513339     0.513339   \n",
      "...              ...            ...            ...           ...          ...   \n",
      "27638       0.666667       0.666667       0.559524      0.559524     0.666667   \n",
      "27639       0.615385       0.615385       0.615385      0.615385     0.615385   \n",
      "27640       0.542493       0.542493       0.542493      0.542493     0.542493   \n",
      "27641       0.529240       0.529240       0.529240      0.529240     0.529240   \n",
      "27642       0.666667       0.534196       0.534196      0.534196     0.750000   \n",
      "\n",
      "       vb_percentage  weekday  \n",
      "0           0.128655        0  \n",
      "1           0.076503        0  \n",
      "2           0.141956        0  \n",
      "3           0.121387        0  \n",
      "4           0.084567        0  \n",
      "...              ...      ...  \n",
      "27638       0.097561        0  \n",
      "27639       0.156757        0  \n",
      "27640       0.076923        0  \n",
      "27641       0.117409        0  \n",
      "27642       0.072607        0  \n",
      "\n",
      "[27643 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.drop(columns=['article', 'h1', 'h2'])\n",
    "test_df = test_df.drop(columns=['article', 'h1', 'h2'])\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 28.1 GiB for an array with shape (27643, 136450) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-6fe324e9e6a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_sparse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_sparse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 28.1 GiB for an array with shape (27643, 136450) and data type float64"
     ]
    }
   ],
   "source": [
    "for i in train_sparse:\n",
    "    train_df = pd.concat([train_df, pd.DataFrame(i.toarray())], axis=1)\n",
    "\n",
    "for i in test_sparse:\n",
    "    test_df = pd.concat([test_df, pd.DataFrame(i.toarray())], axis=1)\n",
    "#print(train_df.columns)\n",
    "X_train = train_df.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])\n",
    "X_test = test_df.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, train_df['label'], test_size=0.33, random_state=42)\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# hash words to 1024 buckets\n",
    "hashvec = HashingVectorizer(n_features=2**14,\n",
    "                            preprocessor=preprocessor,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "# no .fit needed for HashingVectorizer, since it's defined by the hash function\n",
    "\n",
    "# transform sentences to vectors of dimension 1024\n",
    "train_h1_hash = hashvec.transform(train_df['h1'])\n",
    "train_h2_hash = hashvec.transform(train_df['h2'])\n",
    "train_article_hash = hashvec.transform(train_df['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel_hash = hashvec.transform(train_df['channel'])\n",
    "train_con = pd.concat([train_con, pd.DataFrame(train_channel_hash.toarray())], axis=1)\n",
    "X_train = train_con.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])\n",
    "#X_test = test_df.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, train_df['label'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([         'id',   'title_len', 'article_len',      'author',\n",
      "       'author_post',     'channel',  'cnt_iframe',     'cnt_img',\n",
      "           'cnt_vdo',     'dayhour',\n",
      "       ...\n",
      "                1014,          1015,          1016,          1017,\n",
      "                1018,          1019,          1020,          1021,\n",
      "                1022,          1023],\n",
      "      dtype='object', length=3098)\n",
      "Index([    'title_len',   'article_len',        'author',       'channel',\n",
      "             'dayhour', 'jj_percentage',         'label',         'links',\n",
      "       'nn_percentage',    'popularity',\n",
      "       ...\n",
      "                  1014,            1015,            1016,            1017,\n",
      "                  1018,            1019,            1020,            1021,\n",
      "                  1022,            1023],\n",
      "      dtype='object', length=3086)\n"
     ]
    }
   ],
   "source": [
    "train_con = pd.concat([train_df, pd.DataFrame(train_h1_hash.toarray())], axis=1)\n",
    "train_con = pd.concat([train_con, pd.DataFrame(train_h2_hash.toarray())], axis=1)\n",
    "train_con = pd.concat([train_con, pd.DataFrame(train_article_hash.toarray())], axis=1)\n",
    "print(train_con.columns)\n",
    "train_con = train_con.drop(columns=['article', 'h1', 'h2'])\n",
    "X_train = train_con.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])\n",
    "#X_test = test_df.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, train_df['label'], test_size=0.33, random_state=42)\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([         'id',   'title_len',     'article', 'article_len',\n",
      "            'author', 'author_post',     'channel',  'cnt_iframe',\n",
      "           'cnt_img',     'cnt_vdo',\n",
      "       ...\n",
      "                1014,          1015,          1016,          1017,\n",
      "                1018,          1019,          1020,          1021,\n",
      "                1022,          1023],\n",
      "      dtype='object', length=3100)\n"
     ]
    }
   ],
   "source": [
    "test_h1_hash = hashvec.transform(test_df['h1'])\n",
    "test_h2_hash = hashvec.transform(test_df['h2'])\n",
    "test_article_hash = hashvec.transform(test_df['article'])\n",
    "test_con = pd.concat([test_df, pd.DataFrame(test_h1_hash.toarray())], axis=1)\n",
    "test_con = pd.concat([test_con, pd.DataFrame(test_h2_hash.toarray())], axis=1)\n",
    "test_con = pd.concat([test_con, pd.DataFrame(test_article_hash.toarray())], axis=1)\n",
    "print(test_con.columns)\n",
    "test_con = test_con.drop(columns=['article', 'h1', 'h2'])\n",
    "X_test = test_con.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5317380312062959\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipe1 = Pipeline([('author', AuthorTransFormer()),\n",
    "                  ('channel', ChannelTransFormer()),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer()),\n",
    "                  ('topics', TopicsTransFormer()),\n",
    "                  ('std', StandardScaler()),\n",
    "                  (\"pca\", PCA()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.015,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipe1.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipe1.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5410396600048862\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipe1 = Pipeline([('author', AuthorTransFormer()),\n",
    "                  ('channel', ChannelTransFormer()),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer()),\n",
    "                  ('topics', TopicsTransFormer()),\n",
    "                  ('std', StandardScaler()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.015,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipe1.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipe1.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5451525591971237\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipe1 = Pipeline([('author', AuthorTransFormer()),\n",
    "                  ('channel', ChannelTransFormer()),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer()),\n",
    "                  ('topics', TopicsTransFormer()),\n",
    "                  ('std', StandardScaler()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.05,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipe1.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipe1.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5451525591971237\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "pipech = Pipeline([('author', AuthorTransFormer()),\n",
    "                  ('channel', ChannelTransFormer()),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer()),\n",
    "                  ('topics', TopicsTransFormer()),\n",
    "                  ('std', StandardScaler()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.05,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipech.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipech.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44722664 0.3861879  0.53726065 ... 0.44468287 0.6237414  0.6583281 ]\n"
     ]
    }
   ],
   "source": [
    "print(pipe1.predict_proba(X_val)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('author', AuthorTransFormer()),\n",
       "                ('channel', ChannelTransFormer()),\n",
       "                ('topics', TopicsTransFormer()), ('std', StandardScaler()),\n",
       "                ('clf',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, eval_metric='error', gamma=0,\n",
       "                               gpu_id=-1, importance_type='gain',\n",
       "                               interaction_constraints='', learning_rate=0.05,\n",
       "                               max_delta_step=0, max_depth=5,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               subsample=1, tree_method='exact',\n",
       "                               validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_total = train_con.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])\n",
    "pipetotal = Pipeline([('author', AuthorTransFormer()),\n",
    "                      ('channel', ChannelTransFormer()),\n",
    "                      ('author_channel', AuthorChannelPopTransFormer()),\n",
    "                      ('topics', TopicsTransFormer()),\n",
    "                      ('std', StandardScaler()),\n",
    "                      ('clf', XGBClassifier(max_depth=5,learning_rate=0.05,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipetotal.fit(X_total, train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.5 0.4 ... 0.6 0.4 0.7]\n"
     ]
    }
   ],
   "source": [
    "result = pipetotal.predict_proba(X_test)\n",
    "result = result.round(1)\n",
    "print(result[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  Popularity\n",
      "0      27643         0.4\n",
      "1      27644         0.5\n",
      "2      27645         0.4\n",
      "3      27646         0.4\n",
      "4      27647         0.4\n",
      "...      ...         ...\n",
      "11842  39485         0.8\n",
      "11843  39486         0.5\n",
      "11844  39487         0.6\n",
      "11845  39488         0.4\n",
      "11846  39489         0.7\n",
      "\n",
      "[11847 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "submit_ans = pd.DataFrame()\n",
    "submit_ans['Id'] = test_df['id']\n",
    "submit_ans['Popularity'] = result[:,1]\n",
    "\n",
    "print(submit_ans)\n",
    "submit_ans.to_csv('./out.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27638</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27640</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27641</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27642</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27643 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5     6     7     8     9     ...  1014  \\\n",
       "0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "27638   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "27639   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "27640   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "27641   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "27642   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "       1015  1016  1017  1018  1019  1020  1021  1022  1023  \n",
       "0       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "27638   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "27639   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "27640   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "27641   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "27642   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[27643 rows x 1024 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.sparse.from_spmatrix(train_channel_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(preprocessor=<function preprocessor at 0x0000023738CFAA68>,\n",
       "                tokenizer=<function tokenizer_stem_nostop at 0x000002372372FE58>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_words = pd.concat([train_df['channel'], train_df['h1']])\n",
    "tfidf = TfidfVectorizer(preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "tfidf.fit(doc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c6bb81f8a6bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_con\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'author_post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cnt_iframe'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cnt_img'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cnt_vdo'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'topics_pop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'topics_pop_10'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'topics_pop_15'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'topics_pop_20'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'topics_pop_25'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'topics_pop_5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#X_test = test_df.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "train_con = pd.concat([train_df, pd.DataFrame(tfidf_channel.transform(train_df['channel']).toarray())], axis=1)\n",
    "train_con = pd.concat([train_con, pd.DataFrame(tfidf.transform(train_df['h1']).toarray())], axis=1)\n",
    "train_con = pd.concat([train_con, pd.DataFrame.sparse.from_spmatrix(train_h2_hash)], axis=1)\n",
    "train_con = pd.concat([train_con, pd.DataFrame.sparse.from_spmatrix(train_article_hash)], axis=1)\n",
    "train_con = train_con.drop(columns=['article', 'h1', 'h2'])\n",
    "X_train = train_con.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])\n",
    "#X_test = test_df.drop(columns=['id', 'author_post', 'cnt_iframe', 'cnt_img', 'cnt_vdo', 'topics_pop', 'topics_pop_10', 'topics_pop_15', 'topics_pop_20', 'topics_pop_25', 'topics_pop_5', 'title'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, train_df['label'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5456548859148453\n"
     ]
    }
   ],
   "source": [
    "pipenew = Pipeline([('author', AuthorTransFormer()),\n",
    "                  ('channel', ChannelTransFormer()),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer()),\n",
    "                  ('topics', TopicsTransFormer()),\n",
    "                  ('std', StandardScaler()),\n",
    "                  ('clf', XGBClassifier(max_depth=5,learning_rate=0.05,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipenew.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipenew.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5403270726447634\n"
     ]
    }
   ],
   "source": [
    "pipenew2 = Pipeline([('author', AuthorTransFormer()),\n",
    "                  ('channel', ChannelTransFormer()),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer()),\n",
    "                  ('topics', TopicsTransFormer()),\n",
    "                  ('std', StandardScaler()),\n",
    "                  ('clf', XGBClassifier(max_depth=7,learning_rate=0.015,objective=\"binary:logistic\", eval_metric=\"error\"))])\n",
    "\n",
    "pipenew2.fit(X_train, y_train)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipenew2.predict_proba(X_val)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dayhour', 'author_post', 'nn_percentage', 'rb_percentage',\n",
      "       'jj_percentage', 'links', 'popularity', 'article_len', 'vb_percentage',\n",
      "       'topics', 'channel', 'author', 'label', 'channel_pop',\n",
      "       'author_channel_pop', 'author_channel_count'],\n",
      "      dtype='object')\n",
      "Index(['dayhour', 'author_post', 'nn_percentage', 'rb_percentage',\n",
      "       'jj_percentage', 'links', 'popularity', 'article_len', 'vb_percentage',\n",
      "       'topics', 'channel', 'author', 'label', 'channel_pop',\n",
      "       'author_channel_pop', 'author_channel_count'],\n",
      "      dtype='object')\n",
      "0.5277589735894935\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "print(X_train.columns)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, train_df['label'], test_size=0.33, random_state=42)\n",
    "pipenew2 = Pipeline([('author', AuthorTransFormer()),\n",
    "                  ('channel', ChannelTransFormer()),\n",
    "                  ('author_channel', AuthorChannelPopTransFormer()),\n",
    "                  ('topics', TopicsTransFormer()),\n",
    "                  ('std', StandardScaler()),\n",
    "                  ('clf', SVC(kernel='poly',probability=True))])\n",
    "\n",
    "pipenew2.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_val, pipenew2.predict_proba(X_val)[:,1])\n",
    "print(X_train.columns)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testfidf_Xvalt_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-224-f21d6b22b3d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtest_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_channel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_Xval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'channel'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtrain_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_channel_topics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_Xtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'channel'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtest_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_channel_topics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestfidf_Xvalt_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'channel'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mtrain_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_channel_topics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_Xtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topics'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtest_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_channel_topics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_Xval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topics'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testfidf_Xvalt_df' is not defined"
     ]
    }
   ],
   "source": [
    "# TF-IDF Transform\n",
    "text_feature = ['article', 'h1', 'h2']\n",
    "train_sparse = []\n",
    "test_sparse = []\n",
    "for i in text_feature:\n",
    "    train_sparse.append(tfidf.transform(tfidf_Xtrain[i]))\n",
    "    test_sparse.append(tfidf.transform(tfidf_Xval[i]))\n",
    "train_sparse.append(tfidf_channel.transform(tfidf_Xtrain['channel']))\n",
    "test_sparse.append(tfidf_channel.transform(tfidf_Xval['channel']))\n",
    "train_sparse.append(tfidf_channel_topics.transform(tfidf_Xtrain['channel']))\n",
    "test_sparse.append(tfidf_channel_topics.transform(tfidf_Xval['channel']))\n",
    "train_sparse.append(tfidf_channel_topics.transform(tfidf_Xtrain['topics']))\n",
    "test_sparse.append(tfidf_channel_topics.transform(tfidf_Xval['topics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sparse.append(tfidf_channel_topics.transform(tfidf_Xval['channel']))\n",
    "train_sparse.append(tfidf_channel_topics.transform(tfidf_Xtrain['topics']))\n",
    "test_sparse.append(tfidf_channel_topics.transform(tfidf_Xval['topics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "train_sparsematrix = sparse.hstack((train_sparse[0], train_sparse[1]))\n",
    "train_sparsematrix = sparse.hstack((train_sparsematrix, train_sparse[2]))\n",
    "train_sparsematrix2 = sparse.hstack((train_sparsematrix, train_sparse[4]))\n",
    "train_sparsematrix2 = sparse.hstack((train_sparsematrix2, train_sparse[5]))\n",
    "train_sparsematrix = sparse.hstack((train_sparsematrix, train_sparse[3]))\n",
    "train_sparse[0].shape\n",
    "train_sparsematrix.shape\n",
    "train_sparsematrix2.shape\n",
    "test_sparsematrix = sparse.hstack((test_sparse[0], test_sparse[1]))\n",
    "test_sparsematrix = sparse.hstack((test_sparsematrix, test_sparse[2]))\n",
    "test_sparsematrix2 = sparse.hstack((test_sparsematrix, test_sparse[4]))\n",
    "test_sparsematrix2 = sparse.hstack((test_sparsematrix2, test_sparse[5]))\n",
    "test_sparsematrix = sparse.hstack((test_sparsematrix, test_sparse[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5510908936423801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.8, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1)\n",
    "bag.fit(train_sparsematrix, y_train)\n",
    "score = roc_auc_score(y_val, bag.predict_proba(test_sparsematrix)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5548182639281716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag2 = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.8, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag2.fit(train_sparsematrix2, y_train)\n",
    "score = roc_auc_score(y_val, bag2.predict_proba(test_sparsematrix2)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "bag3 = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=2000, \n",
    "                        max_samples=0.8, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag3.fit(train_sparsematrix2, y_train)\n",
    "score = roc_auc_score(y_val, bag3.predict_proba(test_sparsematrix2)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8934907 , 0.4785735 , 0.54578213, ..., 0.3985051 , 0.25326268,\n",
       "       0.59861693])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.predict_proba(X_val_sparse)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5410403092543539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NBarticle = MultinomialNB()\n",
    "NBarticle.fit(X_train_sparse0, y_train)\n",
    "score = roc_auc_score(y_val, NBarticle.predict_proba(X_val_sparse0)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5346228149148473\n"
     ]
    }
   ],
   "source": [
    "NBh1 = MultinomialNB()\n",
    "NBh1.fit(X_train_sparse1, y_train)\n",
    "score = roc_auc_score(y_val, NBh1.predict_proba(X_val_sparse1)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5078712119900506\n"
     ]
    }
   ],
   "source": [
    "NBh2 = MultinomialNB()\n",
    "NBh2.fit(X_train_sparse2, y_train)\n",
    "score = roc_auc_score(y_val, NBh2.predict_proba(X_val_sparse2)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5245936900646555\n"
     ]
    }
   ],
   "source": [
    "NBchannel = MultinomialNB()\n",
    "NBchannel.fit(X_train_sparse3, y_train)\n",
    "score = roc_auc_score(y_val, NBchannel.predict_proba(X_val_sparse3)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "import itertools\n",
    "print('[Voting]')\n",
    "best_vt, best_w, best_score = None, (), -1\n",
    "for a, b, c, d in list(itertools.permutations(range(0,4))):\n",
    "    clf = VotingClassifier(estimators=[('article', NBarticle), ('h1', NBh1), ('h2', NBh2), ('channel', NBchannel)],\n",
    "                            voting='soft', weights=[a,b,c,d])\n",
    "    clf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse, X_val_sparse, y_train, y_val = train_test_split(train_sparsematrix, train_df['label'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembleCV(model1, model2, alpha, val_df, X_val_other, X_val_text):\n",
    "    max = 0\n",
    "    maxpick = 0\n",
    "    l = []\n",
    "    for a in alpha:\n",
    "        pb_list = []\n",
    "        P_1 = model1.predict_proba(X_val_other)[:,1]\n",
    "        P_2 = model2.predict_proba(X_val_text)[:,1]\n",
    "        for d in range(len(val_df)):\n",
    "            p1 = P_1[d]\n",
    "            p2 = P_2[d]\n",
    "            pb_list.append(p1*a + p2*(1-a))\n",
    "        score = roc_auc_score(val_df, pb_list) \n",
    "        if(score > max): \n",
    "            max = score \n",
    "            maxpick = a\n",
    "    return(max, maxpick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5509718543141906, 0.5)\n"
     ]
    }
   ],
   "source": [
    "ans = ensembleCV(pipenew2, bag, [(i / 100) for i in range(0, 100, 10)], y_val, X_val, X_val_sparse)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py:509: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  \"pandas.DataFrame with sparse columns found.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5510571705034906, 0.46)\n"
     ]
    }
   ],
   "source": [
    "ans = ensembleCV(pipenew2, bag, [(i / 200) for i in range(90, 111, 1)], y_val, X_val, X_val_sparse)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_sparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-22676d17a259>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_sparse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_sparse' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_train_sparse[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\bs4\\__init__.py:343: MarkupResemblesLocatorWarning: \"music\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-d70ca56de7be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m                         \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbootstrap_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                         n_jobs=1, random_state=1)\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mbag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_sparse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_sparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \"\"\"\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[0;32m    378\u001b[0m                 \u001b[0mtotal_n_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 verbose=self.verbose)\n\u001b[1;32m--> 380\u001b[1;33m             for i in range(n_jobs))\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# Reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnot_indices_mask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_effective_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m         \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "train_channel_hash = hashvec.transform(train_df['channel'])\n",
    "from scipy import sparse\n",
    "train_sparsematrix = sparse.hstack((train_h1_hash, train_channel_hash))\n",
    "train_sparsematrix = sparse.hstack((train_sparsematrix, train_h2_hash))\n",
    "train_sparsematrix = sparse.hstack((train_sparsematrix, train_article_hash))\n",
    "X_train_sparse, X_val_sparse, y_train, y_val = train_test_split(train_sparsematrix, train_df['label'], test_size=0.33, random_state=42)\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "bag = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.7, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag.fit(X_train_sparse, y_train)\n",
    "score = roc_auc_score(y_val, bag.predict_proba(X_val_sparse)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-4bb545249f1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                         \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbootstrap_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                         n_jobs=1, random_state=1)\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mbag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \"\"\"\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[0;32m    378\u001b[0m                 \u001b[0mtotal_n_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 verbose=self.verbose)\n\u001b[1;32m--> 380\u001b[1;33m             for i in range(n_jobs))\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# Reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnot_indices_mask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_effective_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m         \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program_files\\Anaconda 3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "SC = MaxAbsScaler()\n",
    "SC.fit(X_train_sparse)\n",
    "X_std = SC.transform(X_train_sparse)\n",
    "X_val_std = SC.transform(X_val_sparse)\n",
    "bag = BaggingClassifier(base_estimator=MultinomialNB(), n_estimators=500, \n",
    "                        max_samples=0.7, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=1, random_state=1)\n",
    "bag.fit(X_std, y_train)\n",
    "score = roc_auc_score(y_val, bag.predict_proba(X_val_std)[:,1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AuthorProblem(DF, mode):\n",
    "    if mode == 'train':\n",
    "        new_df = pd.DataFrame([], columns = DF.columns)\n",
    "        for index, row in DF.iterrows(): \n",
    "            morethanone = False\n",
    "            for i in range(len(row['author'].split())):\n",
    "                if(row['author'].find(',')):\n",
    "                    a = row['author'].find(',')\n",
    "                    row['author'] = row['author'][:a] + \" \" + row['author'][a:]\n",
    "                if row['author'].split()[i] == 'and' or row['author'].split()[i] == '&' or row['author'].split()[i] == ',':\n",
    "                    morethanone = True\n",
    "                    a = row.copy(deep = True)\n",
    "                    if(i + 2 < len(row['author'].split())):\n",
    "                        a.loc['author'] = row['author'].split()[i+1] + ' ' + row['author'].split()[i+2]\n",
    "                        new_df = new_df.append(a)\n",
    "        DF = pd.concat([DF, new_df], ignore_index=True)\n",
    "    elif mode == 'test':\n",
    "        for index, row in DF.iterrows():\n",
    "            morethanone = False\n",
    "            for i in range(len(row['author'].split())):\n",
    "                if(row['author'].find(',')):\n",
    "                    a = row['author'].find(',')\n",
    "                    row['author'] = row['author'][:a] + \" \" + row['author'][a:]\n",
    "                if row['author'].split()[i] == 'and' or row['author'].split()[i] == '&' or row['author'].split()[i] == ',':\n",
    "                    morethanone = True\n",
    "                    break\n",
    "            if(morethanone):\n",
    "                if len(row['author'].split()) > 1:\n",
    "                    DF.loc[index,'author'] = row['author'].split()[0] + ' ' + row['author'].split()[1]\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([], columns = train_df.columns).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Clara Moskowitz\n"
     ]
    }
   ],
   "source": [
    "for index, row in train_df.iterrows():\n",
    "    print(index)\n",
    "    print(row['author'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  title_len                                            article  \\\n",
      "0   0          8  There may be killer asteroids headed for Earth...   \n",
      "1   1         12  Google took a stand of sorts against patent-la...   \n",
      "2   2         12  You've spend countless hours training to be an...   \n",
      "3   3          5  Tired of the same old sports fails and news fa...   \n",
      "4   4         10  At 6-foot-5 and 298 pounds, All-Pro NFL star J...   \n",
      "\n",
      "   article_len           author  author_post        channel  cnt_iframe  \\\n",
      "0          577  Clara Moskowitz           18          world           0   \n",
      "1          305          Sam ham          399           tech           0   \n",
      "2         1114        Sam Laird         1004  entertainment          25   \n",
      "3          278        Sam Laird         1004    watercooler          21   \n",
      "4         1370  Connor Finnegan           87  entertainment           1   \n",
      "\n",
      "   cnt_img  cnt_vdo  ...                                              title  \\\n",
      "0        2        0  ...  NASA's Grand Challenge: Stop Asteroids From De...   \n",
      "1        3        0  ...  Google's New Open Source Patent Pledge: We Won...   \n",
      "2       28        0  ...  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
      "3       21        0  ...        Cameraperson Fails Deliver Slapstick Laughs   \n",
      "4      103        0  ...  NFL Star Helps Young Fan Prove Friendship With...   \n",
      "\n",
      "                                              topics topics_pop  \\\n",
      "0  asteroid asteroids challenge earth space u.s. ...   0.610000   \n",
      "1  patent-lawsuit theater apps and software googl...   1.000000   \n",
      "2  nfl espn entertainment nfl nfl draft sports te...   0.700000   \n",
      "3            youtube sports video videos watercooler   0.509849   \n",
      "4  nfl instagram entertainment instagram instagra...   0.513339   \n",
      "\n",
      "   topics_pop_10  topics_pop_15  topics_pop_20  topics_pop_25  topics_pop_5  \\\n",
      "0       0.610000       0.610000       0.610000       0.610000      0.610000   \n",
      "1       0.615385       0.612903       0.612903       0.612903      0.615385   \n",
      "2       0.700000       0.501658       0.501658       0.501658      0.700000   \n",
      "3       0.509849       0.509849       0.509849       0.509849      0.509849   \n",
      "4       0.513339       0.513339       0.513339       0.513339      0.513339   \n",
      "\n",
      "   vb_percentage weekday  \n",
      "0       0.128655       0  \n",
      "1       0.076503       0  \n",
      "2       0.141956       0  \n",
      "3       0.121387       0  \n",
      "4       0.084567       0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_files\\Anaconda 3\\lib\\site-packages\\pandas\\core\\indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "trydf = train_df.head(5)\n",
    "trydf.loc[1,'author'] = 'Sam ham, Sam Sam & Ham Ham'\n",
    "trydf = AuthorProblem(trydf, 'test')\n",
    "print(trydf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Jinyu Chang\"\n",
    "s.find(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "<html>\n",
      " <head>\n",
      "  <div class=\"article-info\">\n",
      "   <span class=\"byline\">\n",
      "    <a href=\"/author/christina/\">\n",
      "     <img alt=\"2016%2f06%2f28%2fa2%2fhttpsd2mhye01h4nj2n.cloudfront.netmediazgkymde1lza0.6e864\" class=\"author_image\" src=\"http://i.amz.mshcdn.com/EOF74clpv5lKQmem8EbdqygnSIA=/90x90/2016%2F06%2F28%2Fa2%2Fhttpsd2mhye01h4nj2n.cloudfront.netmediaZgkyMDE1LzA0.6e864.jpg\"/>\n",
      "    </a>\n",
      "    <span class=\"author_name\">\n",
      "     By\n",
      "     <a href=\"/author/christina/\">\n",
      "      Christina Warren\n",
      "     </a>\n",
      "    </span>\n",
      "    <time datetime=\"Thu, 28 Mar 2013 17:40:55 +0000\">\n",
      "     2013-03-28 17:40:55 UTC\n",
      "    </time>\n",
      "   </span>\n",
      "  </div>\n",
      " </head>\n",
      " <body>\n",
      "  <h1 class=\"title\">\n",
      "   Google's New Open Source Patent Pledge: We Won't Sue Unless Attacked First\n",
      "  </h1>\n",
      "  <figure class=\"article-image\">\n",
      "   <img class=\"microcontent\" data-fragment=\"lead-image\" data-image=\"http://i.amz.mshcdn.com/libxaUsE6Ziezz1FVi_31dND9gs=/950x534/2013%2F03%2F28%2Fd6%2Fpatentmagni.f1555.jpg\" data-micro=\"1\" data-url=\"null\" src=\"http://i.amz.mshcdn.com/libxaUsE6Ziezz1FVi_31dND9gs=/950x534/2013%2F03%2F28%2Fd6%2Fpatentmagni.f1555.jpg\"/>\n",
      "  </figure>\n",
      "  <article data-channel=\"tech\">\n",
      "   <section class=\"article-content\">\n",
      "    <p>\n",
      "     Google took a stand of sorts against\n",
      "     <a href=\"http://mashable.com/category/patent-lawsuit-theater/\">\n",
      "      patent-lawsuit theater\n",
      "     </a>\n",
      "     Thursday with its new\n",
      "     <a href=\"http://www.google.com/patents/opnpledge/\" target=\"_blank\">\n",
      "      Open Patent Non-Assertion (OPN) Pledge\n",
      "     </a>\n",
      "     .\n",
      "    </p>\n",
      "    <p>\n",
      "     As\n",
      "     <a href=\"http://google-opensource.blogspot.co.uk/2013/03/taking-stand-on-open-source-and-patents.html\" target=\"_blank\">\n",
      "      explained by Google's Duane Valz\n",
      "     </a>\n",
      "     , under the OPN Pledge, Google promises \"not to sue any user, distributor or developer of open-source software on specified patents, unless first attacked.\"\n",
      "    </p>\n",
      "    <p>\n",
      "     Now, Google isn't making all of its patents available for others. Instead, its starting small with\n",
      "     <a href=\"http://www.google.com/patents/opnpledge/patents/\" target=\"_blank\">\n",
      "      10 patents\n",
      "     </a>\n",
      "     focused on MapReduce, a programming model for handling large data sets. There are already open-sourced versions of MapReduce available — including Hadoop — that are widely used across the Internet. Google says that over time, it plans to extend the OPN Pledge to more Google patents.\n",
      "    </p>\n",
      "    <p>\n",
      "     <strong>\n",
      "      <div class=\"see-also\">\n",
      "       SEE ALSO:\n",
      "       <a href=\"http://mashable.com/2013/03/27/google-glass-announces-winners/\">\n",
      "        10 Interesting Google Glass Winners and Their Wacky Ideas\n",
      "       </a>\n",
      "      </div>\n",
      "     </strong>\n",
      "    </p>\n",
      "    <p>\n",
      "     The big caveat here is that Google is pledging not to sue anyone who uses its MapReduce patents for Free or Open Source Software. Google is\n",
      "     <a href=\"http://www.google.com/patents/opnpledge/pledge/\" target=\"_blank\">\n",
      "      defining Free or Open Source software\n",
      "     </a>\n",
      "     as any software that meets the Open Source Initiative's \"Open Source Definition,\" as well as any version of the Free Software Foundation's \"Free Software Definition.\" Still, Google iterates that the OPN Pledge isn't limited to a specific project or open-source copyright — as long as the project meets the FSF or OSI's definition for Free Software or Open Source Software, it's protected by the OPN Pledge.\n",
      "    </p>\n",
      "    <p>\n",
      "     Google hopes that its OPN Pledge can be a model for other companies who are looking at how to \"put their own patents into the service of open-source software.\"\n",
      "    </p>\n",
      "    <p>\n",
      "     In the short-term, we're not sure what this does — except indemnify the open source MapReduce projects from a Google lawsuit. The bigger picture, however, is that this could be a new model for the way that patents are applied to open source as a whole. And that's a good thing.\n",
      "    </p>\n",
      "    <p>\n",
      "     <em>\n",
      "      Image courtesy of\n",
      "      <a href=\"http://www.istockphoto.com/mashableoffer.php\" target=\"_blank\">\n",
      "       iStockphoto\n",
      "      </a>\n",
      "      ,\n",
      "      <a href=\"http://www.istockphoto.com/user_view.php?id=3135499\" target=\"_blank\">\n",
      "       stuartbur\n",
      "      </a>\n",
      "     </em>\n",
      "    </p>\n",
      "   </section>\n",
      "  </article>\n",
      "  <footer class=\"article-topics\">\n",
      "   Topics:\n",
      "   <a href=\"/category/apps-software/\">\n",
      "    Apps and Software\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"/category/google/\">\n",
      "    Google\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"/category/open-source/\">\n",
      "    open source\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"/category/opn-pledge/\">\n",
      "    opn pledge\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"/category/patent-lawsuit-theater/\">\n",
      "    patent lawsuit theater\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"/category/software-patents/\">\n",
      "    software patents\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"/category/tech/\">\n",
      "    Tech\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"/category/us/\">\n",
      "    U.S.\n",
      "   </a>\n",
      "  </footer>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "dd = pd.read_csv('./train.csv')\n",
    "print(dd.head(5))\n",
    "soup = BeautifulSoup(dd.loc[1]['Page content'], \"html.parser\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
